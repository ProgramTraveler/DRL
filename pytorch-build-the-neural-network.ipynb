{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Build the neural network","metadata":{}},{"cell_type":"markdown","source":"* Neural networks comprise of layers/modules that perform operations on data. \n* The `torch.nn` namespace provides all the building blocks you need to build your own neural network. \n* Every module in PyTorch subclasses the `nn.Module`. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.","metadata":{}},{"cell_type":"markdown","source":"* In the following sections, we’ll build a neural network to classify images in the FashionMNIST dataset.","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:12:10.348007Z","iopub.execute_input":"2023-10-02T03:12:10.348380Z","iopub.status.idle":"2023-10-02T03:12:14.416098Z","shell.execute_reply.started":"2023-10-02T03:12:10.348336Z","shell.execute_reply":"2023-10-02T03:12:14.415094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get device for training","metadata":{}},{"cell_type":"markdown","source":"* We want to be able to train our model on a hardware accelerator like the GPU or MPS, if available. Let’s check to see if `torch.cuda` or `torch.backends.mps` are available, otherwise we use the CPU.","metadata":{}},{"cell_type":"code","source":"device = (\n\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n\nprint(f\"Using {device} device\")","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:12:14.417985Z","iopub.execute_input":"2023-10-02T03:12:14.418476Z","iopub.status.idle":"2023-10-02T03:12:14.503246Z","shell.execute_reply.started":"2023-10-02T03:12:14.418441Z","shell.execute_reply":"2023-10-02T03:12:14.502468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the class","metadata":{}},{"cell_type":"markdown","source":"* We define our neural network by subclassing` nn.Module`, and initialize the neural network layers in `__init__`. \n* Every `nn.Module` subclass implements the operations on input data in the `forward` method.","metadata":{}},{"cell_type":"code","source":"class NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28 * 28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n        \n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:12:14.504674Z","iopub.execute_input":"2023-10-02T03:12:14.505326Z","iopub.status.idle":"2023-10-02T03:12:14.514234Z","shell.execute_reply.started":"2023-10-02T03:12:14.505289Z","shell.execute_reply":"2023-10-02T03:12:14.513616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We create an instance of `NeuralNetwork`, and move it to the `device`, and print its structure.","metadata":{}},{"cell_type":"code","source":"model = NeuralNetwork().to(device)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:12:14.517004Z","iopub.execute_input":"2023-10-02T03:12:14.518676Z","iopub.status.idle":"2023-10-02T03:12:18.138490Z","shell.execute_reply.started":"2023-10-02T03:12:14.518640Z","shell.execute_reply":"2023-10-02T03:12:18.137481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* To use the model, we pass it the input data. This executes the model’s `forward`, along with some `background operations`. * Do not call model.forward() directly!\n* Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. We get the prediction probabilities by passing it through an instance of the `nn.Softmax` module.","metadata":{}},{"cell_type":"code","source":"X = torch.rand(1, 28, 28, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:12:18.139793Z","iopub.execute_input":"2023-10-02T03:12:18.141694Z","iopub.status.idle":"2023-10-02T03:12:20.762014Z","shell.execute_reply.started":"2023-10-02T03:12:18.141659Z","shell.execute_reply":"2023-10-02T03:12:20.760887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model layers","metadata":{}},{"cell_type":"markdown","source":"* Let’s break down the layers in the FashionMNIST model. \n* To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network.","metadata":{}},{"cell_type":"code","source":"input_image = torch.rand(3, 28, 28)\nprint(input_image.size())","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:12:20.763634Z","iopub.execute_input":"2023-10-02T03:12:20.764001Z","iopub.status.idle":"2023-10-02T03:12:20.770259Z","shell.execute_reply.started":"2023-10-02T03:12:20.763969Z","shell.execute_reply":"2023-10-02T03:12:20.769167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### nn.Flatten","metadata":{}},{"cell_type":"markdown","source":"* We initialize the `nn.Flatten` layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained).","metadata":{}},{"cell_type":"code","source":"flatten = nn.Flatten()\nflat_image = flatten(input_image)\nprint(flat_image.size())","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:12:20.771910Z","iopub.execute_input":"2023-10-02T03:12:20.772419Z","iopub.status.idle":"2023-10-02T03:12:20.783470Z","shell.execute_reply.started":"2023-10-02T03:12:20.772381Z","shell.execute_reply":"2023-10-02T03:12:20.782269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### nn.Linear","metadata":{}},{"cell_type":"markdown","source":"* The `linear layer` is a module that applies a linear transformation on the input using its stored weights and biases.","metadata":{}},{"cell_type":"code","source":"layer1 = nn.Linear(in_features=28*28, out_features=20)\nhidden1 = layer1(flat_image)\nprint(hidden1.size())","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:12:20.785084Z","iopub.execute_input":"2023-10-02T03:12:20.785811Z","iopub.status.idle":"2023-10-02T03:12:20.838840Z","shell.execute_reply.started":"2023-10-02T03:12:20.785776Z","shell.execute_reply":"2023-10-02T03:12:20.837802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### nn.ReLU","metadata":{}},{"cell_type":"markdown","source":"* Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n* In this model, we use nn.ReLU between our linear layers, but there’s other activations to introduce non-linearity in your model.","metadata":{}},{"cell_type":"code","source":"print(f\"Before ReLU: {hidden1}\\n\\n\")\nhidden1 = nn.ReLU()(hidden1)\nprint(f\"After ReLU: {hidden1}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:12:20.840610Z","iopub.execute_input":"2023-10-02T03:12:20.841226Z","iopub.status.idle":"2023-10-02T03:12:20.890249Z","shell.execute_reply.started":"2023-10-02T03:12:20.841191Z","shell.execute_reply":"2023-10-02T03:12:20.889134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### nn.Sequential","metadata":{}},{"cell_type":"markdown","source":"* `nn.Sequential` is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like `seq_modules`.","metadata":{}},{"cell_type":"code","source":"seq_modules = nn.Sequential(\n    flatten,\n    layer1,\n    nn.ReLU(),\n    nn.Linear(20, 10)\n)\n\ninput_image = torch.rand(3, 28, 28)\nlogits = seq_modules(input_image)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:12:20.893658Z","iopub.execute_input":"2023-10-02T03:12:20.894543Z","iopub.status.idle":"2023-10-02T03:12:20.906166Z","shell.execute_reply.started":"2023-10-02T03:12:20.894503Z","shell.execute_reply":"2023-10-02T03:12:20.904890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### nn.Softmax","metadata":{}},{"cell_type":"markdown","source":"* The last linear layer of the neural network returns `logits` - raw values in [-infty, infty] - which are passed to the `nn.Softmax` module. \n* The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. \n* `dim` parameter indicates the dimension along which the values must sum to 1.","metadata":{}},{"cell_type":"code","source":"softmax = nn.Softmax(dim=1)\npred_probab = softmax(logits)\nprint(pred_probab)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:12:20.908283Z","iopub.execute_input":"2023-10-02T03:12:20.909088Z","iopub.status.idle":"2023-10-02T03:12:20.921604Z","shell.execute_reply.started":"2023-10-02T03:12:20.909048Z","shell.execute_reply":"2023-10-02T03:12:20.920221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Parameters","metadata":{}},{"cell_type":"markdown","source":"* Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing `nn.Module` automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s `parameters()` or `named_parameters()` methods.","metadata":{}},{"cell_type":"code","source":"print(f\"Model structure: {model}\\n\\n\")\n\nfor name, param in model.named_parameters():\n    print(f\"Layer: {name} | Size: {param.size()} | Values: {param[:2]}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-10-02T03:12:20.923480Z","iopub.execute_input":"2023-10-02T03:12:20.924125Z","iopub.status.idle":"2023-10-02T03:12:20.967733Z","shell.execute_reply.started":"2023-10-02T03:12:20.924090Z","shell.execute_reply":"2023-10-02T03:12:20.966729Z"},"trusted":true},"execution_count":null,"outputs":[]}]}