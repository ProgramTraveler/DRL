{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ÂáÜÂ§áÂ∑•‰Ωú","metadata":{}},{"cell_type":"markdown","source":"* ÂàõÂª∫ËôöÊãüÂ±èÂπï","metadata":{}},{"cell_type":"code","source":"%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip install pyvirtualdisplay\n!pip install pyglet==1.5.1","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:06:16.150318Z","iopub.execute_input":"2023-09-21T11:06:16.150711Z","iopub.status.idle":"2023-09-21T11:06:46.571137Z","shell.execute_reply.started":"2023-09-21T11:06:16.150677Z","shell.execute_reply":"2023-09-21T11:06:46.569867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:06:46.573538Z","iopub.execute_input":"2023-09-21T11:06:46.573856Z","iopub.status.idle":"2023-09-21T11:06:47.407014Z","shell.execute_reply.started":"2023-09-21T11:06:46.573829Z","shell.execute_reply":"2023-09-21T11:06:47.406025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ÂÆâË£Ö‰æùËµñ","metadata":{}},{"cell_type":"code","source":"!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:06:47.408671Z","iopub.execute_input":"2023-09-21T11:06:47.409327Z","iopub.status.idle":"2023-09-21T11:07:09.746121Z","shell.execute_reply.started":"2023-09-21T11:06:47.409293Z","shell.execute_reply":"2023-09-21T11:07:09.744948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ÂØºÂåÖ","metadata":{}},{"cell_type":"code","source":"# !pip install torch==2.0.1+cu118 -f https://download.pytorch.org/whl/cu118/torch_stable.html","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:09.749652Z","iopub.execute_input":"2023-09-21T11:07:09.750029Z","iopub.status.idle":"2023-09-21T11:07:09.755689Z","shell.execute_reply.started":"2023-09-21T11:07:09.749993Z","shell.execute_reply":"2023-09-21T11:07:09.754136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install gym==0.25.2","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:09.757231Z","iopub.execute_input":"2023-09-21T11:07:09.757652Z","iopub.status.idle":"2023-09-21T11:07:09.768049Z","shell.execute_reply.started":"2023-09-21T11:07:09.757596Z","shell.execute_reply":"2023-09-21T11:07:09.767107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nfrom collections import deque\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n# Gym\nimport gym\nimport gym_pygame\n\n# Hugging Face Hub\nfrom huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\nimport imageio","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:09.769667Z","iopub.execute_input":"2023-09-21T11:07:09.770087Z","iopub.status.idle":"2023-09-21T11:07:12.081402Z","shell.execute_reply.started":"2023-09-21T11:07:09.770056Z","shell.execute_reply":"2023-09-21T11:07:12.080340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Ê£ÄÊü•ÊòØÂê¶‰ΩøÁî® GPU","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.083168Z","iopub.execute_input":"2023-09-21T11:07:12.083754Z","iopub.status.idle":"2023-09-21T11:07:12.158667Z","shell.execute_reply.started":"2023-09-21T11:07:12.083705Z","shell.execute_reply":"2023-09-21T11:07:12.157582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.160601Z","iopub.execute_input":"2023-09-21T11:07:12.161061Z","iopub.status.idle":"2023-09-21T11:07:12.174177Z","shell.execute_reply.started":"2023-09-21T11:07:12.161024Z","shell.execute_reply":"2023-09-21T11:07:12.172954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Agent Áé© CartPole-v1","metadata":{}},{"cell_type":"markdown","source":"## The CartPole-v1 environment\n\n> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n\nSo, we start with CartPole-v1. The goal is to push the cart left or right **so that the pole stays in the equilibrium.**\n\nThe episode ends if:\n- The pole Angle is greater than ¬±12¬∞\n- Cart Position is greater than ¬±2.4\n- Episode length is greater than 500\n\nWe get a reward üí∞ of +1 every timestep the Pole stays in the equilibrium.","metadata":{}},{"cell_type":"code","source":"env_id = \"CartPole-v1\"\n# ÂàõÂª∫ÁéØÂ¢É\nenv = gym.make(env_id)\n\n# ÂàõÂª∫ËØÑ‰º∞ÁéØÂ¢É\neval_env = gym.make(env_id)\n\n# Ëé∑ÂèñÁä∂ÊÄÅÁ©∫Èó¥ÂíåÂä®‰ΩúÁ©∫Èó¥\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.175953Z","iopub.execute_input":"2023-09-21T11:07:12.176483Z","iopub.status.idle":"2023-09-21T11:07:12.195596Z","shell.execute_reply.started":"2023-09-21T11:07:12.176420Z","shell.execute_reply":"2023-09-21T11:07:12.194607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.200062Z","iopub.execute_input":"2023-09-21T11:07:12.200762Z","iopub.status.idle":"2023-09-21T11:07:12.207661Z","shell.execute_reply.started":"2023-09-21T11:07:12.200723Z","shell.execute_reply":"2023-09-21T11:07:12.206472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.209349Z","iopub.execute_input":"2023-09-21T11:07:12.210114Z","iopub.status.idle":"2023-09-21T11:07:12.219732Z","shell.execute_reply.started":"2023-09-21T11:07:12.210053Z","shell.execute_reply":"2023-09-21T11:07:12.218512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ÂàõÂª∫Âº∫ÂåñÊû∂ÊûÑ\n\n* ‰∏§‰∏™ÂÖ®ËøûÊé•Â±Ç fc1 Âíå fc2\n* ‰ΩøÁî® ReLU ‰Ωú‰∏∫ fc1 ÁöÑÊøÄÊ¥ªÂáΩÊï∞\n* ‰ΩøÁî® softmax ËæìÂá∫Âä®‰ΩúÁöÑÊ¶ÇÁéáÂàÜÂ∏É","metadata":{}},{"cell_type":"code","source":"class Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        return F.softmax(x, dim=1)\n    \n    def act(self, state):\n        # torch.from_numpy(state) Â∞Ü state ËΩ¨Êç¢‰∏∫ PyTorch Âº†Èáè\n        # unsqueeze(0) Âú®Âº†ÈáèÁöÑÁ¨¨ 0 Áª¥ÔºàÂç≥ÊúÄÂ§ñÂ±ÇÁª¥Â∫¶Ôºâ‰∏äÊ∑ªÂä†‰∏Ä‰∏™Áª¥Â∫¶ ËøôÊòØÂõ†‰∏∫Á•ûÁªèÁΩëÁªúÈÄöÂ∏∏Ë¶ÅÊ±ÇËæìÂÖ•Êï∞ÊçÆÁöÑÁ¨¨ 0 Áª¥Ë°®Á§∫ÊâπÊ¨°Â§ßÂ∞è\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        # Categorical ÊòØ PyTorch ‰∏≠ÁöÑ‰∏Ä‰∏™ÂàÜÂ∏ÉÁ±ª Áî®‰∫éË°®Á§∫Á¶ªÊï£ÂûãÈöèÊú∫ÂèòÈáèÁöÑÊ¶ÇÁéáÂàÜÂ∏É\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.221437Z","iopub.execute_input":"2023-09-21T11:07:12.222166Z","iopub.status.idle":"2023-09-21T11:07:12.231840Z","shell.execute_reply.started":"2023-09-21T11:07:12.222130Z","shell.execute_reply":"2023-09-21T11:07:12.230988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ÂàõÂª∫Âº∫ÂåñÁÆóÊ≥ï","metadata":{}},{"cell_type":"code","source":"def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    # ËÆ°ÁÆóËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÂàÜÊï∞\n    scores_deque = deque(maxlen=100)\n    scores = []\n    \n    for i_episode in range(1, n_training_episodes + 1):\n        saved_log_probs = []\n        rewards = []\n        state = env.reset()[0] # ÂíåÊ∫ê‰ª£Á†ÅÁöÑ‰∏ÄÁÇπÂå∫Âà´ ‰∏çÁÑ∂Êä•Èîô TypeError: expected np.ndarray (got tuple)\n        \n        for t in range(max_t):\n            action, log_prob = policy.act(state)\n            saved_log_probs.append(log_prob)\n            state, reward, done, _, _ = env.step(action) # ËøôÈáå‰πüÂÅö‰∫Ü‰øÆÊîπ\n            rewards.append(reward)\n            if done:\n                break\n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n        \n        returns = deque(maxlen=max_t)\n        n_steps = len(rewards)\n        \n        for t in range(n_steps)[::-1]:\n            disc_return_t = (returns[0] if len(returns) > 0 else 0)\n            returns.appendleft(gamma * disc_return_t + rewards[t])\n            \n        eps = np.finfo(np.float32).eps.item()\n        \n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n        \n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n        \n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n        \n        if i_episode % print_every == 0:\n            print(\"Episode {}\\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))\n            \n    return scores\n            ","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:10:56.737195Z","iopub.execute_input":"2023-09-21T11:10:56.737579Z","iopub.status.idle":"2023-09-21T11:10:56.749163Z","shell.execute_reply.started":"2023-09-21T11:10:56.737547Z","shell.execute_reply":"2023-09-21T11:10:56.748207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ÂºÄÂßãËÆ≠ÁªÉ","metadata":{}},{"cell_type":"code","source":"cartpole_hyperparameters = {\n    \"h_size\": 16,\n    \"n_training_episodes\": 1000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 1000,\n    \"gamma\": 1.0,\n    \"lr\": 1e-2,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:10:01.653924Z","iopub.execute_input":"2023-09-21T11:10:01.654344Z","iopub.status.idle":"2023-09-21T11:10:01.660380Z","shell.execute_reply.started":"2023-09-21T11:10:01.654311Z","shell.execute_reply":"2023-09-21T11:10:01.658960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create policy and place it to the device\ncartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\ncartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:10:03.114806Z","iopub.execute_input":"2023-09-21T11:10:03.115198Z","iopub.status.idle":"2023-09-21T11:10:03.124333Z","shell.execute_reply.started":"2023-09-21T11:10:03.115162Z","shell.execute_reply":"2023-09-21T11:10:03.123230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = reinforce(cartpole_policy,\n                   cartpole_optimizer,\n                   cartpole_hyperparameters[\"n_training_episodes\"], \n                   cartpole_hyperparameters[\"max_t\"],\n                   cartpole_hyperparameters[\"gamma\"], \n                   100)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:10:59.263120Z","iopub.execute_input":"2023-09-21T11:10:59.263903Z","iopub.status.idle":"2023-09-21T11:25:50.335188Z","shell.execute_reply.started":"2023-09-21T11:10:59.263868Z","shell.execute_reply":"2023-09-21T11:25:50.334154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param policy: The Reinforce agent\n  \"\"\"\n  episode_rewards = []\n  for episode in range(n_eval_episodes):\n    state = env.reset()[0] # ËøôÈáå‰πüÊòØ\n    step = 0\n    done = False\n    total_rewards_ep = 0\n    \n    for step in range(max_steps):\n      action, _ = policy.act(state)\n      new_state, reward, done, info, _ = env.step(action) # ËøôÈáå‰πüÂÅö‰∫Ü‰øÆÊîπ\n      total_rewards_ep += reward\n        \n      if done:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n\n  return mean_reward, std_reward","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:27:33.501030Z","iopub.execute_input":"2023-09-21T11:27:33.501392Z","iopub.status.idle":"2023-09-21T11:27:33.509341Z","shell.execute_reply.started":"2023-09-21T11:27:33.501362Z","shell.execute_reply":"2023-09-21T11:27:33.508394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_agent(eval_env, \n               cartpole_hyperparameters[\"max_t\"], \n               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n               cartpole_policy)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:27:35.481860Z","iopub.execute_input":"2023-09-21T11:27:35.482235Z","iopub.status.idle":"2023-09-21T11:27:41.573565Z","shell.execute_reply.started":"2023-09-21T11:27:35.482202Z","shell.execute_reply":"2023-09-21T11:27:41.572521Z"},"trusted":true},"execution_count":null,"outputs":[]}]}