{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/chhelp/unit-4-code-your-first-deep-reinforcement-learnin?scriptVersionId=143834486\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 准备工作","metadata":{}},{"cell_type":"markdown","source":"* 创建虚拟屏幕","metadata":{}},{"cell_type":"code","source":"%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip install pyvirtualdisplay\n!pip install pyglet==1.5.1","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:06:16.150318Z","iopub.execute_input":"2023-09-21T11:06:16.150711Z","iopub.status.idle":"2023-09-21T11:06:46.571137Z","shell.execute_reply.started":"2023-09-21T11:06:16.150677Z","shell.execute_reply":"2023-09-21T11:06:46.569867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:06:46.573538Z","iopub.execute_input":"2023-09-21T11:06:46.573856Z","iopub.status.idle":"2023-09-21T11:06:47.407014Z","shell.execute_reply.started":"2023-09-21T11:06:46.573829Z","shell.execute_reply":"2023-09-21T11:06:47.406025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 安装依赖","metadata":{}},{"cell_type":"code","source":"!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:06:47.408671Z","iopub.execute_input":"2023-09-21T11:06:47.409327Z","iopub.status.idle":"2023-09-21T11:07:09.746121Z","shell.execute_reply.started":"2023-09-21T11:06:47.409293Z","shell.execute_reply":"2023-09-21T11:07:09.744948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 导包","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nfrom collections import deque\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n# Gym\nimport gym\nimport gym_pygame\n\n# Hugging Face Hub\nfrom huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\nimport imageio","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:09.769667Z","iopub.execute_input":"2023-09-21T11:07:09.770087Z","iopub.status.idle":"2023-09-21T11:07:12.081402Z","shell.execute_reply.started":"2023-09-21T11:07:09.770056Z","shell.execute_reply":"2023-09-21T11:07:12.08034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 检查是否使用 GPU","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.083168Z","iopub.execute_input":"2023-09-21T11:07:12.083754Z","iopub.status.idle":"2023-09-21T11:07:12.158667Z","shell.execute_reply.started":"2023-09-21T11:07:12.083705Z","shell.execute_reply":"2023-09-21T11:07:12.157582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.160601Z","iopub.execute_input":"2023-09-21T11:07:12.161061Z","iopub.status.idle":"2023-09-21T11:07:12.174177Z","shell.execute_reply.started":"2023-09-21T11:07:12.161024Z","shell.execute_reply":"2023-09-21T11:07:12.172954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Agent 玩 CartPole-v1","metadata":{}},{"cell_type":"markdown","source":"## The CartPole-v1 environment\n\n> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n\nSo, we start with CartPole-v1. The goal is to push the cart left or right **so that the pole stays in the equilibrium.**\n\nThe episode ends if:\n- The pole Angle is greater than ±12°\n- Cart Position is greater than ±2.4\n- Episode length is greater than 500\n\nWe get a reward 💰 of +1 every timestep the Pole stays in the equilibrium.","metadata":{}},{"cell_type":"code","source":"env_id = \"CartPole-v1\"\n# 创建环境\nenv = gym.make(env_id)\n\n# 创建评估环境\neval_env = gym.make(env_id)\n\n# 获取状态空间和动作空间\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.175953Z","iopub.execute_input":"2023-09-21T11:07:12.176483Z","iopub.status.idle":"2023-09-21T11:07:12.195596Z","shell.execute_reply.started":"2023-09-21T11:07:12.17642Z","shell.execute_reply":"2023-09-21T11:07:12.194607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.200062Z","iopub.execute_input":"2023-09-21T11:07:12.200762Z","iopub.status.idle":"2023-09-21T11:07:12.207661Z","shell.execute_reply.started":"2023-09-21T11:07:12.200723Z","shell.execute_reply":"2023-09-21T11:07:12.206472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.209349Z","iopub.execute_input":"2023-09-21T11:07:12.210114Z","iopub.status.idle":"2023-09-21T11:07:12.219732Z","shell.execute_reply.started":"2023-09-21T11:07:12.210053Z","shell.execute_reply":"2023-09-21T11:07:12.218512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 创建强化架构\n\n* 两个全连接层 fc1 和 fc2\n* 使用 ReLU 作为 fc1 的激活函数\n* 使用 softmax 输出动作的概率分布","metadata":{}},{"cell_type":"code","source":"class Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        return F.softmax(x, dim=1)\n    \n    def act(self, state):\n        # torch.from_numpy(state) 将 state 转换为 PyTorch 张量\n        # unsqueeze(0) 在张量的第 0 维（即最外层维度）上添加一个维度 这是因为神经网络通常要求输入数据的第 0 维表示批次大小\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        # Categorical 是 PyTorch 中的一个分布类 用于表示离散型随机变量的概率分布\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.221437Z","iopub.execute_input":"2023-09-21T11:07:12.222166Z","iopub.status.idle":"2023-09-21T11:07:12.23184Z","shell.execute_reply.started":"2023-09-21T11:07:12.22213Z","shell.execute_reply":"2023-09-21T11:07:12.230988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 创建强化算法","metadata":{}},{"cell_type":"code","source":"def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    # 计算训练过程中的分数\n    scores_deque = deque(maxlen=100)\n    scores = []\n    \n    for i_episode in range(1, n_training_episodes + 1):\n        saved_log_probs = []\n        rewards = []\n        state = env.reset()[0] # 和源代码的一点区别 不然报错 TypeError: expected np.ndarray (got tuple)\n        \n        for t in range(max_t):\n            action, log_prob = policy.act(state)\n            saved_log_probs.append(log_prob)\n            state, reward, done, _, _ = env.step(action) # 这里也做了修改\n            rewards.append(reward)\n            if done:\n                break\n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n        \n        returns = deque(maxlen=max_t)\n        n_steps = len(rewards)\n        \n        for t in range(n_steps)[::-1]:\n            disc_return_t = (returns[0] if len(returns) > 0 else 0)\n            returns.appendleft(gamma * disc_return_t + rewards[t])\n            \n        eps = np.finfo(np.float32).eps.item()\n        \n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n        \n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n        \n        optimizer.zero_grad() # 清空之间计算的梯度\n        policy_loss.backward() # 计算策略损失对神经网络参数的梯度\n        optimizer.step() # 根据计算到的梯度更新神经网络的参数\n        \n        if i_episode % print_every == 0:\n            print(\"Episode {}\\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))\n            \n    return scores\n            ","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:10:56.737195Z","iopub.execute_input":"2023-09-21T11:10:56.737579Z","iopub.status.idle":"2023-09-21T11:10:56.749163Z","shell.execute_reply.started":"2023-09-21T11:10:56.737547Z","shell.execute_reply":"2023-09-21T11:10:56.748207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 开始训练","metadata":{}},{"cell_type":"code","source":"cartpole_hyperparameters = {\n    \"h_size\": 16,\n    \"n_training_episodes\": 1000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 1000,\n    \"gamma\": 1.0,\n    \"lr\": 1e-2,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:10:01.653924Z","iopub.execute_input":"2023-09-21T11:10:01.654344Z","iopub.status.idle":"2023-09-21T11:10:01.66038Z","shell.execute_reply.started":"2023-09-21T11:10:01.654311Z","shell.execute_reply":"2023-09-21T11:10:01.65896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create policy and place it to the device\ncartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\ncartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:10:03.114806Z","iopub.execute_input":"2023-09-21T11:10:03.115198Z","iopub.status.idle":"2023-09-21T11:10:03.124333Z","shell.execute_reply.started":"2023-09-21T11:10:03.115162Z","shell.execute_reply":"2023-09-21T11:10:03.12323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = reinforce(cartpole_policy,\n                   cartpole_optimizer,\n                   cartpole_hyperparameters[\"n_training_episodes\"], \n                   cartpole_hyperparameters[\"max_t\"],\n                   cartpole_hyperparameters[\"gamma\"], \n                   100)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:10:59.26312Z","iopub.execute_input":"2023-09-21T11:10:59.263903Z","iopub.status.idle":"2023-09-21T11:25:50.335188Z","shell.execute_reply.started":"2023-09-21T11:10:59.263868Z","shell.execute_reply":"2023-09-21T11:25:50.334154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param policy: The Reinforce agent\n  \"\"\"\n  episode_rewards = []\n  for episode in range(n_eval_episodes):\n    state = env.reset()[0] # 这里也是\n    step = 0\n    done = False\n    total_rewards_ep = 0\n    \n    for step in range(max_steps):\n      action, _ = policy.act(state)\n      new_state, reward, done, info, _ = env.step(action) # 这里也做了修改\n      total_rewards_ep += reward\n        \n      if done:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n\n  return mean_reward, std_reward","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:27:33.50103Z","iopub.execute_input":"2023-09-21T11:27:33.501392Z","iopub.status.idle":"2023-09-21T11:27:33.509341Z","shell.execute_reply.started":"2023-09-21T11:27:33.501362Z","shell.execute_reply":"2023-09-21T11:27:33.508394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_agent(eval_env, \n               cartpole_hyperparameters[\"max_t\"], \n               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n               cartpole_policy)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:27:35.48186Z","iopub.execute_input":"2023-09-21T11:27:35.482235Z","iopub.status.idle":"2023-09-21T11:27:41.573565Z","shell.execute_reply.started":"2023-09-21T11:27:35.482202Z","shell.execute_reply":"2023-09-21T11:27:41.572521Z"},"trusted":true},"execution_count":null,"outputs":[]}]}