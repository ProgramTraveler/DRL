{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/chhelp/unit-4-code-your-first-deep-reinforcement-learnin?scriptVersionId=143834486\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# å‡†å¤‡å·¥ä½œ","metadata":{}},{"cell_type":"markdown","source":"* åˆ›å»ºè™šæ‹Ÿå±å¹•","metadata":{}},{"cell_type":"code","source":"%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip install pyvirtualdisplay\n!pip install pyglet==1.5.1","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:06:16.150318Z","iopub.execute_input":"2023-09-21T11:06:16.150711Z","iopub.status.idle":"2023-09-21T11:06:46.571137Z","shell.execute_reply.started":"2023-09-21T11:06:16.150677Z","shell.execute_reply":"2023-09-21T11:06:46.569867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:06:46.573538Z","iopub.execute_input":"2023-09-21T11:06:46.573856Z","iopub.status.idle":"2023-09-21T11:06:47.407014Z","shell.execute_reply.started":"2023-09-21T11:06:46.573829Z","shell.execute_reply":"2023-09-21T11:06:47.406025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* å®‰è£…ä¾èµ–","metadata":{}},{"cell_type":"code","source":"!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:06:47.408671Z","iopub.execute_input":"2023-09-21T11:06:47.409327Z","iopub.status.idle":"2023-09-21T11:07:09.746121Z","shell.execute_reply.started":"2023-09-21T11:06:47.409293Z","shell.execute_reply":"2023-09-21T11:07:09.744948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* å¯¼åŒ…","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nfrom collections import deque\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n# Gym\nimport gym\nimport gym_pygame\n\n# Hugging Face Hub\nfrom huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\nimport imageio","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:09.769667Z","iopub.execute_input":"2023-09-21T11:07:09.770087Z","iopub.status.idle":"2023-09-21T11:07:12.081402Z","shell.execute_reply.started":"2023-09-21T11:07:09.770056Z","shell.execute_reply":"2023-09-21T11:07:12.08034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ GPU","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.083168Z","iopub.execute_input":"2023-09-21T11:07:12.083754Z","iopub.status.idle":"2023-09-21T11:07:12.158667Z","shell.execute_reply.started":"2023-09-21T11:07:12.083705Z","shell.execute_reply":"2023-09-21T11:07:12.157582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.160601Z","iopub.execute_input":"2023-09-21T11:07:12.161061Z","iopub.status.idle":"2023-09-21T11:07:12.174177Z","shell.execute_reply.started":"2023-09-21T11:07:12.161024Z","shell.execute_reply":"2023-09-21T11:07:12.172954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Agent ç© CartPole-v1","metadata":{}},{"cell_type":"markdown","source":"## The CartPole-v1 environment\n\n> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n\nSo, we start with CartPole-v1. The goal is to push the cart left or right **so that the pole stays in the equilibrium.**\n\nThe episode ends if:\n- The pole Angle is greater than Â±12Â°\n- Cart Position is greater than Â±2.4\n- Episode length is greater than 500\n\nWe get a reward ğŸ’° of +1 every timestep the Pole stays in the equilibrium.","metadata":{}},{"cell_type":"code","source":"env_id = \"CartPole-v1\"\n# åˆ›å»ºç¯å¢ƒ\nenv = gym.make(env_id)\n\n# åˆ›å»ºè¯„ä¼°ç¯å¢ƒ\neval_env = gym.make(env_id)\n\n# è·å–çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.175953Z","iopub.execute_input":"2023-09-21T11:07:12.176483Z","iopub.status.idle":"2023-09-21T11:07:12.195596Z","shell.execute_reply.started":"2023-09-21T11:07:12.17642Z","shell.execute_reply":"2023-09-21T11:07:12.194607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.200062Z","iopub.execute_input":"2023-09-21T11:07:12.200762Z","iopub.status.idle":"2023-09-21T11:07:12.207661Z","shell.execute_reply.started":"2023-09-21T11:07:12.200723Z","shell.execute_reply":"2023-09-21T11:07:12.206472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.209349Z","iopub.execute_input":"2023-09-21T11:07:12.210114Z","iopub.status.idle":"2023-09-21T11:07:12.219732Z","shell.execute_reply.started":"2023-09-21T11:07:12.210053Z","shell.execute_reply":"2023-09-21T11:07:12.218512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## åˆ›å»ºå¼ºåŒ–æ¶æ„\n\n* ä¸¤ä¸ªå…¨è¿æ¥å±‚ fc1 å’Œ fc2\n* ä½¿ç”¨ ReLU ä½œä¸º fc1 çš„æ¿€æ´»å‡½æ•°\n* ä½¿ç”¨ softmax è¾“å‡ºåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ","metadata":{}},{"cell_type":"code","source":"class Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        return F.softmax(x, dim=1)\n    \n    def act(self, state):\n        # torch.from_numpy(state) å°† state è½¬æ¢ä¸º PyTorch å¼ é‡\n        # unsqueeze(0) åœ¨å¼ é‡çš„ç¬¬ 0 ç»´ï¼ˆå³æœ€å¤–å±‚ç»´åº¦ï¼‰ä¸Šæ·»åŠ ä¸€ä¸ªç»´åº¦ è¿™æ˜¯å› ä¸ºç¥ç»ç½‘ç»œé€šå¸¸è¦æ±‚è¾“å…¥æ•°æ®çš„ç¬¬ 0 ç»´è¡¨ç¤ºæ‰¹æ¬¡å¤§å°\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        # Categorical æ˜¯ PyTorch ä¸­çš„ä¸€ä¸ªåˆ†å¸ƒç±» ç”¨äºè¡¨ç¤ºç¦»æ•£å‹éšæœºå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒ\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:07:12.221437Z","iopub.execute_input":"2023-09-21T11:07:12.222166Z","iopub.status.idle":"2023-09-21T11:07:12.23184Z","shell.execute_reply.started":"2023-09-21T11:07:12.22213Z","shell.execute_reply":"2023-09-21T11:07:12.230988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## åˆ›å»ºå¼ºåŒ–ç®—æ³•","metadata":{}},{"cell_type":"code","source":"def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    # è®¡ç®—è®­ç»ƒè¿‡ç¨‹ä¸­çš„åˆ†æ•°\n    scores_deque = deque(maxlen=100)\n    scores = []\n    \n    for i_episode in range(1, n_training_episodes + 1):\n        saved_log_probs = []\n        rewards = []\n        state = env.reset()[0] # å’Œæºä»£ç çš„ä¸€ç‚¹åŒºåˆ« ä¸ç„¶æŠ¥é”™ TypeError: expected np.ndarray (got tuple)\n        \n        for t in range(max_t):\n            action, log_prob = policy.act(state)\n            saved_log_probs.append(log_prob)\n            state, reward, done, _, _ = env.step(action) # è¿™é‡Œä¹Ÿåšäº†ä¿®æ”¹\n            rewards.append(reward)\n            if done:\n                break\n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n        \n        returns = deque(maxlen=max_t)\n        n_steps = len(rewards)\n        \n        for t in range(n_steps)[::-1]:\n            disc_return_t = (returns[0] if len(returns) > 0 else 0)\n            returns.appendleft(gamma * disc_return_t + rewards[t])\n            \n        eps = np.finfo(np.float32).eps.item()\n        \n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n        \n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n        \n        optimizer.zero_grad() # æ¸…ç©ºä¹‹é—´è®¡ç®—çš„æ¢¯åº¦\n        policy_loss.backward() # è®¡ç®—ç­–ç•¥æŸå¤±å¯¹ç¥ç»ç½‘ç»œå‚æ•°çš„æ¢¯åº¦\n        optimizer.step() # æ ¹æ®è®¡ç®—åˆ°çš„æ¢¯åº¦æ›´æ–°ç¥ç»ç½‘ç»œçš„å‚æ•°\n        \n        if i_episode % print_every == 0:\n            print(\"Episode {}\\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))\n            \n    return scores\n            ","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:10:56.737195Z","iopub.execute_input":"2023-09-21T11:10:56.737579Z","iopub.status.idle":"2023-09-21T11:10:56.749163Z","shell.execute_reply.started":"2023-09-21T11:10:56.737547Z","shell.execute_reply":"2023-09-21T11:10:56.748207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# å¼€å§‹è®­ç»ƒ","metadata":{}},{"cell_type":"code","source":"cartpole_hyperparameters = {\n    \"h_size\": 16,\n    \"n_training_episodes\": 1000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 1000,\n    \"gamma\": 1.0,\n    \"lr\": 1e-2,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:10:01.653924Z","iopub.execute_input":"2023-09-21T11:10:01.654344Z","iopub.status.idle":"2023-09-21T11:10:01.66038Z","shell.execute_reply.started":"2023-09-21T11:10:01.654311Z","shell.execute_reply":"2023-09-21T11:10:01.65896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create policy and place it to the device\ncartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\ncartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:10:03.114806Z","iopub.execute_input":"2023-09-21T11:10:03.115198Z","iopub.status.idle":"2023-09-21T11:10:03.124333Z","shell.execute_reply.started":"2023-09-21T11:10:03.115162Z","shell.execute_reply":"2023-09-21T11:10:03.12323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = reinforce(cartpole_policy,\n                   cartpole_optimizer,\n                   cartpole_hyperparameters[\"n_training_episodes\"], \n                   cartpole_hyperparameters[\"max_t\"],\n                   cartpole_hyperparameters[\"gamma\"], \n                   100)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:10:59.26312Z","iopub.execute_input":"2023-09-21T11:10:59.263903Z","iopub.status.idle":"2023-09-21T11:25:50.335188Z","shell.execute_reply.started":"2023-09-21T11:10:59.263868Z","shell.execute_reply":"2023-09-21T11:25:50.334154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param policy: The Reinforce agent\n  \"\"\"\n  episode_rewards = []\n  for episode in range(n_eval_episodes):\n    state = env.reset()[0] # è¿™é‡Œä¹Ÿæ˜¯\n    step = 0\n    done = False\n    total_rewards_ep = 0\n    \n    for step in range(max_steps):\n      action, _ = policy.act(state)\n      new_state, reward, done, info, _ = env.step(action) # è¿™é‡Œä¹Ÿåšäº†ä¿®æ”¹\n      total_rewards_ep += reward\n        \n      if done:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n\n  return mean_reward, std_reward","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:27:33.50103Z","iopub.execute_input":"2023-09-21T11:27:33.501392Z","iopub.status.idle":"2023-09-21T11:27:33.509341Z","shell.execute_reply.started":"2023-09-21T11:27:33.501362Z","shell.execute_reply":"2023-09-21T11:27:33.508394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_agent(eval_env, \n               cartpole_hyperparameters[\"max_t\"], \n               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n               cartpole_policy)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:27:35.48186Z","iopub.execute_input":"2023-09-21T11:27:35.482235Z","iopub.status.idle":"2023-09-21T11:27:41.573565Z","shell.execute_reply.started":"2023-09-21T11:27:35.482202Z","shell.execute_reply":"2023-09-21T11:27:41.572521Z"},"trusted":true},"execution_count":null,"outputs":[]}]}