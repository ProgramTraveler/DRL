{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/chhelp/deep-reinforcement-learning-lesson4?scriptVersionId=143377684\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport gym\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\n\nclass ConnectFourGym(gym.Env):\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(1,self.rows,self.columns), dtype=int)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n    def reset(self):\n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)\n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1/42\n            return 1/(self.rows*self.columns)\n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:59:31.34517Z","iopub.execute_input":"2023-09-18T07:59:31.345616Z","iopub.status.idle":"2023-09-18T07:59:32.259127Z","shell.execute_reply.started":"2023-09-18T07:59:31.345583Z","shell.execute_reply":"2023-09-18T07:59:32.258455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create ConnectFour environment \nenv = ConnectFourGym(agent2=\"random\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:59:32.260706Z","iopub.execute_input":"2023-09-18T07:59:32.261296Z","iopub.status.idle":"2023-09-18T07:59:32.32728Z","shell.execute_reply.started":"2023-09-18T07:59:32.261259Z","shell.execute_reply":"2023-09-18T07:59:32.325764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch as th\nimport torch.nn as nn\n\n!pip install \"stable-baselines3\"\nfrom stable_baselines3 import PPO \nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\n# Neural network for predicting action values\nclass CustomCNN(BaseFeaturesExtractor):\n    \n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int=128):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n        # CxHxW images (channels first)\n        n_input_channels = observation_space.shape[0]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        # Compute shape by doing one forward pass\n        with th.no_grad():\n            n_flatten = self.cnn(\n                th.as_tensor(observation_space.sample()[None]).float()\n            ).shape[1]\n\n        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n\npolicy_kwargs = dict(\n    features_extractor_class=CustomCNN,\n)\n        \n# Initialize agent\nmodel = PPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:59:32.328851Z","iopub.execute_input":"2023-09-18T07:59:32.329224Z","iopub.status.idle":"2023-09-18T07:59:56.517629Z","shell.execute_reply.started":"2023-09-18T07:59:32.329194Z","shell.execute_reply":"2023-09-18T07:59:56.515199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train agent\nmodel.learn(total_timesteps=60000)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:59:56.51867Z","iopub.status.idle":"2023-09-18T07:59:56.519103Z","shell.execute_reply.started":"2023-09-18T07:59:56.518861Z","shell.execute_reply":"2023-09-18T07:59:56.518886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def agent1(obs, config):\n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:59:56.520359Z","iopub.status.idle":"2023-09-18T07:59:56.520668Z","shell.execute_reply.started":"2023-09-18T07:59:56.520531Z","shell.execute_reply":"2023-09-18T07:59:56.520545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the game environment\nenv = make(\"connectx\")\n\n# Two random agents play one game round\nenv.run([agent1, \"random\"])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:59:56.521789Z","iopub.status.idle":"2023-09-18T07:59:56.522083Z","shell.execute_reply.started":"2023-09-18T07:59:56.521928Z","shell.execute_reply":"2023-09-18T07:59:56.521942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_win_percentages(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:59:56.524131Z","iopub.status.idle":"2023-09-18T07:59:56.524681Z","shell.execute_reply.started":"2023-09-18T07:59:56.524413Z","shell.execute_reply":"2023-09-18T07:59:56.524461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_win_percentages(agent1=agent1, agent2=\"random\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T07:59:56.525971Z","iopub.status.idle":"2023-09-18T07:59:56.526402Z","shell.execute_reply.started":"2023-09-18T07:59:56.526193Z","shell.execute_reply":"2023-09-18T07:59:56.526212Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
