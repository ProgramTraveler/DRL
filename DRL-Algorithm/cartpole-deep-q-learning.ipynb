{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/chhelp/cartpole-deep-q-learning?scriptVersionId=144933652\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Cartpole-Deep-Q-Learning","metadata":{}},{"cell_type":"markdown","source":"* [Source code address](https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/tree/master/Cartpole-Deep-Q-Learning) and thanks to the author for sharing\n* My work: made some modifications(very few) based on the source code and can run it on the notebook","metadata":{}},{"cell_type":"markdown","source":"## re-install gym","metadata":{}},{"cell_type":"markdown","source":"* This code is three years ago, so you need to lower the version of gym","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y gym","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:06:22.586677Z","iopub.execute_input":"2023-10-02T06:06:22.587501Z","iopub.status.idle":"2023-10-02T06:06:25.851331Z","shell.execute_reply.started":"2023-10-02T06:06:22.587468Z","shell.execute_reply":"2023-10-02T06:06:25.84999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gym==0.13.1","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:06:25.85385Z","iopub.execute_input":"2023-10-02T06:06:25.854188Z","iopub.status.idle":"2023-10-02T06:06:43.901897Z","shell.execute_reply.started":"2023-10-02T06:06:25.85416Z","shell.execute_reply":"2023-10-02T06:06:43.900537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show gym","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:06:43.903655Z","iopub.execute_input":"2023-10-02T06:06:43.904013Z","iopub.status.idle":"2023-10-02T06:06:52.59966Z","shell.execute_reply.started":"2023-10-02T06:06:43.903976Z","shell.execute_reply":"2023-10-02T06:06:52.598509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a virtual display","metadata":{}},{"cell_type":"markdown","source":"* create a virtual display ensures that this code(`env = gym.wrappers.Monitor(env, directory=\"monitors\", force=True)`) runs correctly","metadata":{}},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install -y python3-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip3 install pyvirtualdisplay","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:06:52.602579Z","iopub.execute_input":"2023-10-02T06:06:52.602928Z","iopub.status.idle":"2023-10-02T06:07:21.323281Z","shell.execute_reply.started":"2023-10-02T06:06:52.602892Z","shell.execute_reply":"2023-10-02T06:07:21.32211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 虚拟屏幕\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:07:21.325312Z","iopub.execute_input":"2023-10-02T06:07:21.325693Z","iopub.status.idle":"2023-10-02T06:07:22.334422Z","shell.execute_reply.started":"2023-10-02T06:07:21.325655Z","shell.execute_reply":"2023-10-02T06:07:22.333385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Packages","metadata":{}},{"cell_type":"markdown","source":"* In the source code, these classes are in different packages.\n* I extracted them for convenience","metadata":{}},{"cell_type":"markdown","source":"### model.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass QNetwork(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim) -> None:\n        \"\"\"DQN Network\n\n        Args:\n            input_dim (int): `state` dimension.\n                `state` is 2-D tensor of shape (n, input_dim)\n            output_dim (int): Number of actions.\n                Q_value is 2-D tensor of shape (n, output_dim)\n            hidden_dim (int): Hidden dimension in fc layer\n        \"\"\"\n    \n        super(QNetwork, self).__init__()\n\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.Linear(input_dim, hidden_dim),\n#            torch.nn.BatchNorm1d(hidden_dim),\n            torch.nn.PReLU()\n        )\n\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim, hidden_dim),\n            torch.nn.PReLU()\n        )\n\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim, hidden_dim),\n            torch.nn.PReLU()\n        )\n\n        self.final = torch.nn.Linear(hidden_dim, output_dim)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Returns a Q_value\n\n        Args:\n            x (torch.Tensor): `State` 2-D tensor of shape (n, input_dim)\n\n        Returns:\n            torch.Tensor: Q_value, 2-D tensor of shape (n, output_dim)            \n        \"\"\"\n        \n        ## print('type(x) of forward:', type(x))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.final(x)\n\n        return x\n     ","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:07:22.335942Z","iopub.execute_input":"2023-10-02T06:07:22.336829Z","iopub.status.idle":"2023-10-02T06:07:23.642948Z","shell.execute_reply.started":"2023-10-02T06:07:22.336793Z","shell.execute_reply":"2023-10-02T06:07:23.642032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### replay_buffer.py","metadata":{}},{"cell_type":"code","source":"''' https://github.com/pytorch/tutorials/blob/master/intermediate_source/reinforcement_q_learning.py\n    https://gist.github.com/Pocuston/13f1a7786648e1e2ff95bfad02a51521 \n'''\n\n######################################################################\n# Replay Memory\n# -------------\n#\n# We'll be using experience replay memory for training our DQN. It stores\n# the transitions that the agent observes, allowing us to reuse this data\n# later. By sampling from it randomly, the transitions that build up a\n# batch are decorrelated. It has been shown that this greatly stabilizes\n# and improves the DQN training procedure.\n#\n# For this, we're going to need two classses:\n#\n# -  ``Transition`` - a named tuple representing a single transition in\n#    our environment. It essentially maps (state, action) pairs\n#    to their (next_state, reward) result, with the state being the\n#    screen difference image as described later on.\n# -  ``ReplayMemory`` - a cyclic buffer of bounded size that holds the\n#    transitions observed recently. It also implements a ``.sample()``\n#    method for selecting a random batch of transitions for training.\n#\n\nimport random\nfrom collections import namedtuple\n\nTransition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n\nclass ReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        self.position = 0\n       \n    def push(self, batch):\n        self.memory.append(batch)\n        if len(self.memory) > self.capacity:\n            del self.memory[0]\n       \n\n    def sample(self, batch_size): # Remove correlation between batches through random sampling\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:07:23.644292Z","iopub.execute_input":"2023-10-02T06:07:23.644793Z","iopub.status.idle":"2023-10-02T06:07:23.653092Z","shell.execute_reply.started":"2023-10-02T06:07:23.644755Z","shell.execute_reply":"2023-10-02T06:07:23.651933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### agent.py","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64  \nLEARNING_RATE = 0.001\n\nimport torch\nimport torch.optim as optim\nimport random\n# from model import QNetwork\n\nuse_cuda = torch.cuda.is_available()\nFloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nfrom torch.autograd import Variable\n\n# from replay_buffer import ReplayMemory, Transition\n\nclass Agent(object):\n\n    def __init__(self, n_states, n_actions, hidden_dim):\n        \"\"\"Agent class that choose action and train\n\n        Args:\n            n_states (int): input dimension\n            n_actions (int): output dimension\n            hidden_dim (int): hidden dimension\n        \"\"\"\n        \n        self.q_local = QNetwork(n_states, n_actions, hidden_dim=16).to(device)\n        self.q_target = QNetwork(n_states, n_actions, hidden_dim=16).to(device)\n        \n        # create a Mean Squared Error loss function object\n        self.mse_loss = torch.nn.MSELoss()\n        self.optim = optim.Adam(self.q_local.parameters(), lr=LEARNING_RATE)\n        \n        self.n_states = n_states\n        self.n_actions = n_actions\n\n        #  ReplayMemory: trajectory is saved here\n        self.replay_memory = ReplayMemory(10000)\n        \n\n    def get_action(self, state, eps, check_eps=True): # exploitation and exploration trade-off\n        \"\"\"Returns an action\n\n        Args:\n            state : 2-D tensor of shape (n, input_dim)\n            eps (float): eps-greedy for exploration\n\n        Returns: int: action index\n        \"\"\"\n        \n        # declare a global variable that tracks the number of steps passed during the execution of the code\n        global steps_done\n        sample = random.random()\n\n        if check_eps==False or sample > eps: # exploitation\n           with torch.no_grad():\n               # t.max(1) will return largest column value of each row.\n               # second column on max result is index of where max element was\n               # found, so we pick action with the larger expected reward.\n               return self.q_local(Variable(state).type(FloatTensor)).data.max(1)[1].view(1, 1)\n        else: # exploration\n           # return LongTensor([[random.randrange(2)]])\n           return torch.tensor([[random.randrange(self.n_actions)]], device=device) # generate a random action index\n\n\n    def learn(self, experiences, gamma):\n        \"\"\"Prepare minibatch and train them\n\n        Args:\n        experiences (List[Transition]): batch of `Transition`\n        gamma (float): Discount rate of Q_target\n        \"\"\"\n        \n        if len(self.replay_memory.memory) < BATCH_SIZE:\n            return;\n            \n        transitions = self.replay_memory.sample(BATCH_SIZE)\n        \n        batch = Transition(*zip(*transitions))\n\n        states = torch.cat(batch.state)\n        actions = torch.cat(batch.action)\n        rewards = torch.cat(batch.reward)\n        next_states = torch.cat(batch.next_state)\n        dones = torch.cat(batch.done)\n        \n            \n        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n        # columns of actions taken. These are the actions which would've been taken\n        # for each batch state according to newtork q_local (current estimate)\n        Q_expected = self.q_local(states).gather(1, actions)     \n\n        Q_targets_next = self.q_target(next_states).detach().max(1)[0] \n\n        Q_targets = rewards + (gamma * Q_targets_next * (1-dones))\n        \n        #self.q_local.train(mode=True)        \n        self.optim.zero_grad()\n        loss = self.mse_loss(Q_expected, Q_targets.unsqueeze(1))\n        loss.backward()\n        self.optim.step()\n                                   ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-02T06:07:23.65475Z","iopub.execute_input":"2023-10-02T06:07:23.655082Z","iopub.status.idle":"2023-10-02T06:07:23.738593Z","shell.execute_reply.started":"2023-10-02T06:07:23.655051Z","shell.execute_reply":"2023-10-02T06:07:23.737517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The main code","metadata":{}},{"cell_type":"code","source":"import gym\nimport numpy as np\nimport torch\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport time\n\nfrom collections import deque\n# from agent import Agent, FloatTensor\n# from replay_buffer import ReplayMemory, Transition\nfrom  torch.autograd import Variable\n\n# set up matplotlib\nis_ipython = 'inline' in matplotlib.get_backend()\nif is_ipython:\n    from IPython import display\n\nplt.ion()\n\nuse_cuda = torch.cuda.is_available()\nFloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\nBATCH_SIZE = 64  \nTAU = 0.005 # 1e-3   # for soft update of target parameters\ngamma = 0.99\nLEARNING_RATE = 0.001\nTARGET_UPDATE = 10\n\n# num_episodes = 5000\nnum_episodes = 300 # i just want to know how the code work, so the num_episodes is small\nprint_every = 10\nhidden_dim = 16 ## 12 ## 32 ## 16 ## 64 ## 16\nmin_eps = 0.01\nmax_eps_episode = 50\n\nenv = gym.make('CartPole-v1')\nenv = gym.wrappers.Monitor(env, directory=\"monitors\", force=True)\n        \nspace_dim =  env.observation_space.shape[0] # n_spaces\naction_dim = env.action_space.n # n_actions\nprint('input_dim: ', space_dim, ', output_dim: ', action_dim, ', hidden_dim: ', hidden_dim)\n\nthreshold = env.spec.reward_threshold\nprint('threshold: ', threshold)\n\nagent = Agent(space_dim, action_dim, hidden_dim)\n\ndef epsilon_annealing(i_epsiode, max_episode, min_eps: float):\n    ##  if i_epsiode --> max_episode, ret_eps --> min_eps\n    ##  if i_epsiode --> 1, ret_eps --> 1 \n    slope = (min_eps - 1.0) / max_episode\n    ret_eps = max(slope * i_epsiode + 1.0, min_eps)\n    return ret_eps\n\ndef save(directory, filename):\n    torch.save(agent.q_local.state_dict(), '%s/%s_local.pth' % (directory, filename))\n    torch.save(agent.q_target.state_dict(), '%s/%s_target.pth' % (directory, filename))","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:07:23.740271Z","iopub.execute_input":"2023-10-02T06:07:23.740757Z","iopub.status.idle":"2023-10-02T06:07:28.327715Z","shell.execute_reply.started":"2023-10-02T06:07:23.740711Z","shell.execute_reply":"2023-10-02T06:07:28.32676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_episode(env, agent, eps):\n    \"\"\"Play an epsiode and train\n\n    Args:\n        env (gym.Env): gym environment (CartPole-v0)\n        agent (Agent): agent will train and get action        \n        eps (float): eps-greedy for exploration\n\n    Returns:\n        int: reward earned in this episode\n    \"\"\"\n    \n    state = env.reset()\n    done = False\n    total_reward = 0\n    \n    env.render()\n    \n    while not done:\n\n        action = agent.get_action(FloatTensor([state]) , eps)\n        \n        next_state, reward, done, _ = env.step(action.item())\n\n        total_reward += reward\n\n        if done:\n            reward = -1\n                    \n        # Store the transition in memory\n        agent.replay_memory.push(\n                (FloatTensor([state]), \n                 action, # action is already a tensor\n                 FloatTensor([reward]), \n                 FloatTensor([next_state]), \n                 FloatTensor([done])))\n                 \n\n        if len(agent.replay_memory) > BATCH_SIZE:\n\n            batch = agent.replay_memory.sample(BATCH_SIZE)\n            \n            agent.learn(batch, gamma)\n\n        state = next_state\n\n\n    return total_reward","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:07:28.33058Z","iopub.execute_input":"2023-10-02T06:07:28.331385Z","iopub.status.idle":"2023-10-02T06:07:28.338882Z","shell.execute_reply.started":"2023-10-02T06:07:28.331351Z","shell.execute_reply":"2023-10-02T06:07:28.338027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train():    \n\n    scores_deque = deque(maxlen=100)\n    scores_array = []\n    avg_scores_array = []    \n    \n    time_start = time.time()\n\n    for i_episode in range(num_episodes):\n        eps = epsilon_annealing(i_episode, max_eps_episode, min_eps)\n        score = run_episode(env, agent, eps)\n\n        scores_deque.append(score)\n        scores_array.append(score)\n        \n        avg_score = np.mean(scores_deque)\n        avg_scores_array.append(avg_score)\n\n        dt = (int)(time.time() - time_start)\n            \n        if i_episode % print_every == 0 and i_episode > 0:\n            print('Episode: {:5} Score: {:5}  Avg.Score: {:.2f}, eps-greedy: {:5.2f} Time: {:02}:{:02}:{:02}'.\\\n                    format(i_episode, score, avg_score, eps, dt//3600, dt%3600//60, dt%60))\n            \n        if len(scores_deque) == scores_deque.maxlen:\n            ### 195.0: for cartpole-v0 and 475 for v1\n            if np.mean(scores_deque) >= threshold: \n                print('\\n Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'. \\\n                    format(i_episode, np.mean(scores_deque)))\n                break\n\n        if i_episode % TARGET_UPDATE == 0: # updateto a certain number of times, set \"q_local\" to \"q_target\"\n            agent.q_target.load_state_dict(agent.q_local.state_dict()) \n    \n    return scores_array, avg_scores_array","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:07:28.340673Z","iopub.execute_input":"2023-10-02T06:07:28.34133Z","iopub.status.idle":"2023-10-02T06:07:28.363925Z","shell.execute_reply.started":"2023-10-02T06:07:28.341299Z","shell.execute_reply":"2023-10-02T06:07:28.36311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Deep-Q-Learning Agent","metadata":{}},{"cell_type":"code","source":"scores, avg_scores = train()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:07:28.365242Z","iopub.execute_input":"2023-10-02T06:07:28.365602Z","iopub.status.idle":"2023-10-02T06:28:38.361292Z","shell.execute_reply.started":"2023-10-02T06:07:28.36557Z","shell.execute_reply":"2023-10-02T06:28:38.35994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nprint('length of scores: ', len(scores), ', len of avg_scores: ', len(avg_scores))\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nplt.plot(np.arange(1, len(scores)+1), scores, label=\"Score\")\nplt.plot(np.arange(1, len(avg_scores)+1), avg_scores, label=\"Avg on 100 episodes\")\nplt.legend(bbox_to_anchor=(1.05, 1)) \nplt.ylabel('Score')\nplt.xlabel('Episodes #')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:28:38.36301Z","iopub.execute_input":"2023-10-02T06:28:38.363385Z","iopub.status.idle":"2023-10-02T06:28:38.767347Z","shell.execute_reply.started":"2023-10-02T06:28:38.363351Z","shell.execute_reply":"2023-10-02T06:28:38.766503Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
