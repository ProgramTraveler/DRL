{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/chhelp/unit-6-advantage-actor-critic-a2c?scriptVersionId=143840295\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# åˆ›å»ºè™šæ‹Ÿå±å¹•","metadata":{}},{"cell_type":"code","source":"%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip3 install pyvirtualdisplay","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:06:13.599365Z","iopub.execute_input":"2023-09-22T07:06:13.600598Z","iopub.status.idle":"2023-09-22T07:06:34.979283Z","shell.execute_reply.started":"2023-09-22T07:06:13.600554Z","shell.execute_reply":"2023-09-22T07:06:34.977704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:06:34.98253Z","iopub.execute_input":"2023-09-22T07:06:34.983248Z","iopub.status.idle":"2023-09-22T07:06:35.654637Z","shell.execute_reply.started":"2023-09-22T07:06:34.983209Z","shell.execute_reply":"2023-09-22T07:06:35.653549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# å®‰è£…ä¾èµ–\n\n* panda-gym åŒ…å«æœºå™¨äººæ‰‹è‡‚ç¯å¢ƒ\n* stable-baselines3 SB3 æ·±åº¦å­¦ä¹ ç¯å¢ƒ","metadata":{}},{"cell_type":"code","source":"!pip install stable-baselines3[extra]\n!pip install gymnasium","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:06:35.656567Z","iopub.execute_input":"2023-09-22T07:06:35.65693Z","iopub.status.idle":"2023-09-22T07:07:21.230718Z","shell.execute_reply.started":"2023-09-22T07:06:35.656885Z","shell.execute_reply":"2023-09-22T07:07:21.22945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install huggingface_sb3\n!pip install huggingface_hub\n!pip install panda_gym","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:07:21.233906Z","iopub.execute_input":"2023-09-22T07:07:21.234387Z","iopub.status.idle":"2023-09-22T07:13:13.750343Z","shell.execute_reply.started":"2023-09-22T07:07:21.234347Z","shell.execute_reply":"2023-09-22T07:13:13.749019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# å¯¼åŒ…","metadata":{}},{"cell_type":"code","source":"import os\n\nimport gymnasium as gym\nimport panda_gym\n\nfrom huggingface_sb3 import load_from_hub, package_to_hub\n\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\nfrom stable_baselines3.common.env_util import make_vec_env\n\nfrom huggingface_hub import notebook_login","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:13:13.753194Z","iopub.execute_input":"2023-09-22T07:13:13.7539Z","iopub.status.idle":"2023-09-22T07:13:27.858609Z","shell.execute_reply.started":"2023-09-22T07:13:13.753866Z","shell.execute_reply":"2023-09-22T07:13:27.857591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PandaReachDense-v3 ğŸ¦¾","metadata":{}},{"cell_type":"code","source":"# åˆ›å»ºç¯å¢ƒ\nenv_id = \"PandaReachDense-v3\"\n\nenv = gym.make(env_id)\n\n# çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´\ns_size = env.observation_space.shape\na_size = env.action_space","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:13:27.860047Z","iopub.execute_input":"2023-09-22T07:13:27.860737Z","iopub.status.idle":"2023-09-22T07:13:28.324987Z","shell.execute_reply.started":"2023-09-22T07:13:27.860707Z","shell.execute_reply":"2023-09-22T07:13:28.323906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The observation space **is a dictionary with 3 different elements**:\n- `achieved_goal`: (x,y,z) position of the goal.\n- `desired_goal`: (x,y,z) distance between the goal position and the current object position.\n- `observation`: position (x,y,z) and velocity of the end-effector (vx, vy, vz).","metadata":{}},{"cell_type":"code","source":"print(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:13:28.326903Z","iopub.execute_input":"2023-09-22T07:13:28.327289Z","iopub.status.idle":"2023-09-22T07:13:28.336675Z","shell.execute_reply.started":"2023-09-22T07:13:28.327251Z","shell.execute_reply":"2023-09-22T07:13:28.335508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The action space is a vector with 3 values:\n- Control x, y, z movement","metadata":{}},{"cell_type":"code","source":"print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:13:28.338201Z","iopub.execute_input":"2023-09-22T07:13:28.339135Z","iopub.status.idle":"2023-09-22T07:13:28.34757Z","shell.execute_reply.started":"2023-09-22T07:13:28.339096Z","shell.execute_reply":"2023-09-22T07:13:28.346459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## å½’ä¸€åŒ– observatin å’Œ rewards","metadata":{}},{"cell_type":"code","source":"# åˆ›å»ºå¤šä¸ªç›¸åŒçš„ç¯å¢ƒå®ä¾‹ ç»„æˆä¸€ä¸ªç¯å¢ƒé›†åˆ è¿™ä¸ªç¯å¢ƒé›†åˆå¯ä»¥åŒæ—¶è¿è¡Œå¤šä¸ªç¯å¢ƒå®ä¾‹\nenv = make_vec_env(env_id, n_envs=4)\n# å¯¹ç¯å¢ƒé›†åˆä¸­çš„ observation å’Œ rewards è¿›è¡Œå½’ä¸€åŒ–å¤„ç†\n# env æ˜¯ä¸€ä¸ªç¯å¢ƒé›†åˆå®ä¾‹\n# norm_obs=True è¡¨ç¤ºå¯¹è§‚æµ‹å€¼è¿›è¡Œå½’ä¸€åŒ–å¤„ç†\n# norm_reward=True è¡¨ç¤ºå¯¹å¥–åŠ±å€¼è¿›è¡Œå½’ä¸€åŒ–å¤„ç†\n# clip_obs=10 è¡¨ç¤ºå¯¹è§‚æµ‹å€¼è¿›è¡Œæˆªæ–­å¤„ç† ä»¥é¿å…å‡ºç°è¿‡å¤§æˆ–è¿‡å°çš„è§‚æµ‹å€¼ (clip_obs=10 é™åˆ¶åœ¨[-10, 10] çš„èŒƒå›´å†…)\nenv = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:15:34.086905Z","iopub.execute_input":"2023-09-22T07:15:34.087327Z","iopub.status.idle":"2023-09-22T07:15:34.737672Z","shell.execute_reply.started":"2023-09-22T07:15:34.087294Z","shell.execute_reply":"2023-09-22T07:15:34.736567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## åˆ›å»º A2C(Advantage Actor-Critic) æ¨¡å‹","metadata":{}},{"cell_type":"code","source":"# verbose å‚æ•°ç”¨äºæ§åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¾“å‡ºä¿¡æ¯ å½“ verbose=1 æ—¶ æ¨¡å‹ä¼šè¾“å‡ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸€äº›ä¿¡æ¯\nmodel = A2C(policy = \"MultiInputPolicy\",\n            env = env,\n            verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:15:37.650775Z","iopub.execute_input":"2023-09-22T07:15:37.651157Z","iopub.status.idle":"2023-09-22T07:15:43.637444Z","shell.execute_reply.started":"2023-09-22T07:15:37.651126Z","shell.execute_reply":"2023-09-22T07:15:43.636392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# è®­ç»ƒ A2C Agent","metadata":{}},{"cell_type":"code","source":"# model.learn(1_000_000)\n# è¿™é‡Œæˆ‘åªæ˜¯æµ‹è¯• æ‰€ä»¥ episodes æ¯”è¾ƒå°\nmodel.learn(1_000)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:16:25.687134Z","iopub.execute_input":"2023-09-22T07:16:25.687505Z","iopub.status.idle":"2023-09-22T07:16:28.544647Z","shell.execute_reply.started":"2023-09-22T07:16:25.687474Z","shell.execute_reply":"2023-09-22T07:16:28.5435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model and  VecNormalize statistics when saving the agent\nmodel.save(\"a2c-PandaReachDense-v3\")\nenv.save(\"vec_normalize.pkl\")","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:16:30.884794Z","iopub.execute_input":"2023-09-22T07:16:30.885173Z","iopub.status.idle":"2023-09-22T07:16:30.906878Z","shell.execute_reply.started":"2023-09-22T07:16:30.885142Z","shell.execute_reply":"2023-09-22T07:16:30.90571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate the agent ğŸ“ˆ\n- Now that's our  agent is trained, we need to **check its performance**.\n- Stable-Baselines3 provides a method to do that: `evaluate_policy`","metadata":{}},{"cell_type":"code","source":"from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n\n# Load the saved statistics\neval_env = DummyVecEnv([lambda: gym.make(\"PandaReachDense-v3\")])\neval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n\n# We need to override the render_mode\neval_env.render_mode = \"rgb_array\"\n\n#  do not update them at test time\neval_env.training = False\n# reward normalization is not needed at test time\neval_env.norm_reward = False\n\n# Load the agent\nmodel = A2C.load(\"a2c-PandaReachDense-v3\")\n\nmean_reward, std_reward = evaluate_policy(model, eval_env)\n\nprint(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:16:35.385869Z","iopub.execute_input":"2023-09-22T07:16:35.386253Z","iopub.status.idle":"2023-09-22T07:16:37.321483Z","shell.execute_reply.started":"2023-09-22T07:16:35.386222Z","shell.execute_reply":"2023-09-22T07:16:37.320307Z"},"trusted":true},"execution_count":null,"outputs":[]}]}