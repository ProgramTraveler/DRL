{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/chhelp/unit-6-advantage-actor-critic-a2c?scriptVersionId=143840295\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 创建虚拟屏幕","metadata":{}},{"cell_type":"code","source":"%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip3 install pyvirtualdisplay","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:06:13.599365Z","iopub.execute_input":"2023-09-22T07:06:13.600598Z","iopub.status.idle":"2023-09-22T07:06:34.979283Z","shell.execute_reply.started":"2023-09-22T07:06:13.600554Z","shell.execute_reply":"2023-09-22T07:06:34.977704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:06:34.98253Z","iopub.execute_input":"2023-09-22T07:06:34.983248Z","iopub.status.idle":"2023-09-22T07:06:35.654637Z","shell.execute_reply.started":"2023-09-22T07:06:34.983209Z","shell.execute_reply":"2023-09-22T07:06:35.653549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 安装依赖\n\n* panda-gym 包含机器人手臂环境\n* stable-baselines3 SB3 深度学习环境","metadata":{}},{"cell_type":"code","source":"!pip install stable-baselines3[extra]\n!pip install gymnasium","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:06:35.656567Z","iopub.execute_input":"2023-09-22T07:06:35.65693Z","iopub.status.idle":"2023-09-22T07:07:21.230718Z","shell.execute_reply.started":"2023-09-22T07:06:35.656885Z","shell.execute_reply":"2023-09-22T07:07:21.22945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install huggingface_sb3\n!pip install huggingface_hub\n!pip install panda_gym","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:07:21.233906Z","iopub.execute_input":"2023-09-22T07:07:21.234387Z","iopub.status.idle":"2023-09-22T07:13:13.750343Z","shell.execute_reply.started":"2023-09-22T07:07:21.234347Z","shell.execute_reply":"2023-09-22T07:13:13.749019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 导包","metadata":{}},{"cell_type":"code","source":"import os\n\nimport gymnasium as gym\nimport panda_gym\n\nfrom huggingface_sb3 import load_from_hub, package_to_hub\n\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\nfrom stable_baselines3.common.env_util import make_vec_env\n\nfrom huggingface_hub import notebook_login","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:13:13.753194Z","iopub.execute_input":"2023-09-22T07:13:13.7539Z","iopub.status.idle":"2023-09-22T07:13:27.858609Z","shell.execute_reply.started":"2023-09-22T07:13:13.753866Z","shell.execute_reply":"2023-09-22T07:13:27.857591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PandaReachDense-v3 🦾","metadata":{}},{"cell_type":"code","source":"# 创建环境\nenv_id = \"PandaReachDense-v3\"\n\nenv = gym.make(env_id)\n\n# 状态空间和动作空间\ns_size = env.observation_space.shape\na_size = env.action_space","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:13:27.860047Z","iopub.execute_input":"2023-09-22T07:13:27.860737Z","iopub.status.idle":"2023-09-22T07:13:28.324987Z","shell.execute_reply.started":"2023-09-22T07:13:27.860707Z","shell.execute_reply":"2023-09-22T07:13:28.323906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The observation space **is a dictionary with 3 different elements**:\n- `achieved_goal`: (x,y,z) position of the goal.\n- `desired_goal`: (x,y,z) distance between the goal position and the current object position.\n- `observation`: position (x,y,z) and velocity of the end-effector (vx, vy, vz).","metadata":{}},{"cell_type":"code","source":"print(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:13:28.326903Z","iopub.execute_input":"2023-09-22T07:13:28.327289Z","iopub.status.idle":"2023-09-22T07:13:28.336675Z","shell.execute_reply.started":"2023-09-22T07:13:28.327251Z","shell.execute_reply":"2023-09-22T07:13:28.335508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The action space is a vector with 3 values:\n- Control x, y, z movement","metadata":{}},{"cell_type":"code","source":"print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:13:28.338201Z","iopub.execute_input":"2023-09-22T07:13:28.339135Z","iopub.status.idle":"2023-09-22T07:13:28.34757Z","shell.execute_reply.started":"2023-09-22T07:13:28.339096Z","shell.execute_reply":"2023-09-22T07:13:28.346459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 归一化 observatin 和 rewards","metadata":{}},{"cell_type":"code","source":"# 创建多个相同的环境实例 组成一个环境集合 这个环境集合可以同时运行多个环境实例\nenv = make_vec_env(env_id, n_envs=4)\n# 对环境集合中的 observation 和 rewards 进行归一化处理\n# env 是一个环境集合实例\n# norm_obs=True 表示对观测值进行归一化处理\n# norm_reward=True 表示对奖励值进行归一化处理\n# clip_obs=10 表示对观测值进行截断处理 以避免出现过大或过小的观测值 (clip_obs=10 限制在[-10, 10] 的范围内)\nenv = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:15:34.086905Z","iopub.execute_input":"2023-09-22T07:15:34.087327Z","iopub.status.idle":"2023-09-22T07:15:34.737672Z","shell.execute_reply.started":"2023-09-22T07:15:34.087294Z","shell.execute_reply":"2023-09-22T07:15:34.736567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 创建 A2C(Advantage Actor-Critic) 模型","metadata":{}},{"cell_type":"code","source":"# verbose 参数用于控制训练过程中的输出信息 当 verbose=1 时 模型会输出训练过程中的一些信息\nmodel = A2C(policy = \"MultiInputPolicy\",\n            env = env,\n            verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:15:37.650775Z","iopub.execute_input":"2023-09-22T07:15:37.651157Z","iopub.status.idle":"2023-09-22T07:15:43.637444Z","shell.execute_reply.started":"2023-09-22T07:15:37.651126Z","shell.execute_reply":"2023-09-22T07:15:43.636392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 训练 A2C Agent","metadata":{}},{"cell_type":"code","source":"# model.learn(1_000_000)\n# 这里我只是测试 所以 episodes 比较小\nmodel.learn(1_000)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:16:25.687134Z","iopub.execute_input":"2023-09-22T07:16:25.687505Z","iopub.status.idle":"2023-09-22T07:16:28.544647Z","shell.execute_reply.started":"2023-09-22T07:16:25.687474Z","shell.execute_reply":"2023-09-22T07:16:28.5435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model and  VecNormalize statistics when saving the agent\nmodel.save(\"a2c-PandaReachDense-v3\")\nenv.save(\"vec_normalize.pkl\")","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:16:30.884794Z","iopub.execute_input":"2023-09-22T07:16:30.885173Z","iopub.status.idle":"2023-09-22T07:16:30.906878Z","shell.execute_reply.started":"2023-09-22T07:16:30.885142Z","shell.execute_reply":"2023-09-22T07:16:30.90571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate the agent 📈\n- Now that's our  agent is trained, we need to **check its performance**.\n- Stable-Baselines3 provides a method to do that: `evaluate_policy`","metadata":{}},{"cell_type":"code","source":"from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n\n# Load the saved statistics\neval_env = DummyVecEnv([lambda: gym.make(\"PandaReachDense-v3\")])\neval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n\n# We need to override the render_mode\neval_env.render_mode = \"rgb_array\"\n\n#  do not update them at test time\neval_env.training = False\n# reward normalization is not needed at test time\neval_env.norm_reward = False\n\n# Load the agent\nmodel = A2C.load(\"a2c-PandaReachDense-v3\")\n\nmean_reward, std_reward = evaluate_policy(model, eval_env)\n\nprint(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:16:35.385869Z","iopub.execute_input":"2023-09-22T07:16:35.386253Z","iopub.status.idle":"2023-09-22T07:16:37.321483Z","shell.execute_reply.started":"2023-09-22T07:16:35.386222Z","shell.execute_reply":"2023-09-22T07:16:37.320307Z"},"trusted":true},"execution_count":null,"outputs":[]}]}