{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/chhelp/unit-2-q-learning-with-frozenlake-v1-and-taxi-v?scriptVersionId=143490188\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# é€šè¿‡ Q-learning ç®€å•è®­ç»ƒæ¨¡å‹\n\n* gymnasium: åŒ…å« FrozenLake-v1 â›„ å’Œ Taxi-v3 ğŸš• ç¯å¢ƒ\n* pygame: ç”¨äº FrozenLake-v1 å’Œ Taxi-v3 UI\n* numpy: ç”¨äºå¤„ç†æˆ‘ä»¬çš„ Q è¡¨ã€‚","metadata":{}},{"cell_type":"code","source":"!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:07:54.659534Z","iopub.execute_input":"2023-09-19T06:07:54.66Z","iopub.status.idle":"2023-09-19T06:08:21.027299Z","shell.execute_reply.started":"2023-09-19T06:07:54.659966Z","shell.execute_reply":"2023-09-19T06:08:21.025884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install -y python3-opengl\n!apt install ffmpeg xvfb\n!pip3 install pyvirtualdisplay","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:08:21.029921Z","iopub.execute_input":"2023-09-19T06:08:21.030286Z","iopub.status.idle":"2023-09-19T06:08:54.877064Z","shell.execute_reply.started":"2023-09-19T06:08:21.030252Z","shell.execute_reply":"2023-09-19T06:08:54.875484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# åˆ›å»ºè™šæ‹Ÿå±å¹•\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:01.575434Z","iopub.execute_input":"2023-09-19T06:09:01.575996Z","iopub.status.idle":"2023-09-19T06:09:02.06681Z","shell.execute_reply.started":"2023-09-19T06:09:01.575947Z","shell.execute_reply":"2023-09-19T06:09:02.065448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport gymnasium as gym\nimport random # ç”Ÿæˆéšæœºæ•°\nimport imageio # ç”Ÿæˆé‡æ’­è§†é¢‘\nimport tqdm\n\nimport pickle5 as pickle\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:04.419447Z","iopub.execute_input":"2023-09-19T06:09:04.419951Z","iopub.status.idle":"2023-09-19T06:09:05.125752Z","shell.execute_reply.started":"2023-09-19T06:09:04.419913Z","shell.execute_reply":"2023-09-19T06:09:05.124286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# å¼€å§‹è®­ç»ƒ  Frozen Lake â›„\n\nWe're going to train our Q-Learning agent **to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)**.\n\nWe can have two sizes of environment:\n\n- `map_name=\"4x4\"`: a 4x4 grid version\n- `map_name=\"8x8\"`: a 8x8 grid version\n\n\nThe environment has two modes:\n\n- `is_slippery=False`: The agent always moves **in the intended direction** due to the non-slippery nature of the frozen lake (deterministic).\n- `is_slippery=True`: The agent **may not always move in the intended direction** due to the slippery nature of the frozen lake (stochastic).","metadata":{}},{"cell_type":"code","source":"# é€‰æ‹© 4x4 çš„åœ°å›¾\n# å†°æ¹–é€‰æ‹©é˜²æ»‘ ä¿è¯ agent å§‹ç»ˆæœç€é¢„æœŸæ–¹å‘å‰è¿›\n# render_mode æŒ‡å®šå¦‚ä½•å¯è§†åŒ–ç¯å¢ƒ rgb_array -> è¿”å›ç¯å¢ƒå½“å‰çŠ¶æ€çš„å•ä¸ªå¸§ å¸§æ˜¯ä¸€ä¸ª np.ndarray å½¢çŠ¶ä¸º (x, y, 3) è¡¨ç¤º x*y åƒç´ å›¾åƒçš„ RGB å€¼\nenv = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:08.726459Z","iopub.execute_input":"2023-09-19T06:09:08.726904Z","iopub.status.idle":"2023-09-19T06:09:08.757633Z","shell.execute_reply.started":"2023-09-19T06:09:08.726871Z","shell.execute_reply":"2023-09-19T06:09:08.755821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ä½ ä¹Ÿå¯ä»¥è‡ªå®šä¹‰ç½‘æ ¼\n\n\n```python\ndesc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\ngym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n```\n\n* åœ¨è¿™é‡Œæˆ‘ä½¿ç”¨çš„æ˜¯é»˜è®¤ç¯å¢ƒ","metadata":{}},{"cell_type":"code","source":"# æŸ¥çœ‹åˆ›å»ºçš„ç¯å¢ƒ\nprint(\"_____OBSERVATION SPACE_____\")\nprint(\"Observation Space\", env.observation_space)\nprint(\"Sample observation\", env.observation_space.sample()) # å¯¹çŠ¶æ€éšæœºé‡‡æ ·","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:11.690412Z","iopub.execute_input":"2023-09-19T06:09:11.691135Z","iopub.status.idle":"2023-09-19T06:09:11.702793Z","shell.execute_reply.started":"2023-09-19T06:09:11.691079Z","shell.execute_reply":"2023-09-19T06:09:11.700639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* è¾“å‡ºç»“æœä¸­ Observation Space Discrete(16) æ˜¯ä¸€ä¸ªæ•´æ•° å®ƒè¡¨ç¤ºä»£ç†çš„å½“å‰ä½ç½® current_row * ncols + current_col(å…¶ä¸­ row å’Œ col éƒ½æ˜¯ä» 0 å¼€å§‹)\n* åœ¨ 4x4 çš„åœ°å›¾ä¸­ ç›®æ ‡ä½ç½®é€šè¿‡ $3 * 4 + 3 = 15$ è®¡ç®—å¾—åˆ° å¯èƒ½çš„è§‚å¯Ÿå€¼(observations)æ•°é‡å–å†³äºåœ°å›¾çš„å¤§å° 4x4 åœ°å›¾ä¸­æœ‰ 16ä¸ªå¯èƒ½çš„è§‚å¯Ÿå€¼","metadata":{}},{"cell_type":"code","source":"# antion\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample()) # éšæœºé‡‡æ ·ä¸€ä¸ªåŠ¨ä½œ","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:13.678903Z","iopub.execute_input":"2023-09-19T06:09:13.67936Z","iopub.status.idle":"2023-09-19T06:09:13.687184Z","shell.execute_reply.started":"2023-09-19T06:09:13.679326Z","shell.execute_reply":"2023-09-19T06:09:13.685615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*  ç¦»æ•£çš„åŠ¨ä½œç©ºé—´\n\n    -with 4 actions available ğŸ®:\n    - 0: GO LEFT\n    - 1: GO DOWN\n    - 2: GO RIGHT\n    - 3: GO UP\n\n* å¥–åŠ±å‡½æ•°\n\n    -Reward function ğŸ’°:\n    - Reach goal: +1\n    - Reach hole: 0\n    - Reach frozen: 0","metadata":{}},{"cell_type":"code","source":"state_space = env.observation_space.n # çŠ¶æ€ç©ºé—´\nprint(\"There are \", state_space, \"possible states\")\n\naction_space = env.action_space.n\nprint(\"There are \", action_space, \"possible actions\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:16.758112Z","iopub.execute_input":"2023-09-19T06:09:16.758518Z","iopub.status.idle":"2023-09-19T06:09:16.766088Z","shell.execute_reply.started":"2023-09-19T06:09:16.758473Z","shell.execute_reply":"2023-09-19T06:09:16.764399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# åˆ›å»ºå¹¶åˆå§‹åŒ– Q-table\ndef initialize_q_table(state_space, action_space):\n    Qtable = np.zeros((state_space, action_space)) # åˆ›å»ºä¸€ä¸ª åˆå§‹ä¸º 0 å¤§å°ä¸º state_space * action_space çš„å‘é‡\n    return Qtable","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:19.357051Z","iopub.execute_input":"2023-09-19T06:09:19.357433Z","iopub.status.idle":"2023-09-19T06:09:19.363884Z","shell.execute_reply.started":"2023-09-19T06:09:19.357404Z","shell.execute_reply":"2023-09-19T06:09:19.362376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Qtable_frozenlake = initialize_q_table(state_space, action_space)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:21.42235Z","iopub.execute_input":"2023-09-19T06:09:21.422743Z","iopub.status.idle":"2023-09-19T06:09:21.428903Z","shell.execute_reply.started":"2023-09-19T06:09:21.422714Z","shell.execute_reply":"2023-09-19T06:09:21.427447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# å®šä¹‰ greedy ç­–ç•¥ å…¶å®å°±æ˜¯è´ªå¿ƒ\ndef greedy_policy(Qtable, state):\n    action = np.argmax(Qtable[state][:]) # è¿”å›åœ¨å½“å‰çŠ¶æ€ä¸‹ é‡‡å–å“ªä¸ªè¡ŒåŠ¨(è·å–å¥–åŠ±æœ€å¤§çš„é‚£ä¸ªè¡ŒåŠ¨)\n    return action","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:10:57.47236Z","iopub.execute_input":"2023-09-19T06:10:57.472873Z","iopub.status.idle":"2023-09-19T06:10:57.479621Z","shell.execute_reply.started":"2023-09-19T06:10:57.472833Z","shell.execute_reply":"2023-09-19T06:10:57.478338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The idea with epsilon-greedy:\n\n- With *probability 1â€Š-â€ŠÉ›* : **we do exploitation** (i.e. our agent selects the action with the highest state-action pair value).\n\n- With *probability É›*: we do **exploration** (trying a random action).","metadata":{}},{"cell_type":"code","source":"# å®šä¹‰ epsilon-greedy ç­–ç•¥\n# å¤„ç† æ¢ç´¢(exploration)å’Œåˆ©ç”¨(exploitation)ä¹‹é—´çš„å¹³è¡¡\ndef epsilon_greedy_policy(Qtable, state, epsilon):\n    # åœ¨ 0 å’Œ 1 ä¹‹é—´éšæœºäº§ç”Ÿä¸€ä¸ªæ•°å­—\n    random_num = random.uniform(0, 1)\n    \n    if random_num > epsilon: # åˆ©ç”¨ å…¶å®å°±æ˜¯åˆ©ç”¨ä¹‹å‰çš„ Q-table ä¸­çš„ä¿¡æ¯\n        # é€‰æ‹©å½“å‰çŠ¶æ€ä¸‹å¯ä»¥è·å¾—æœ€å¤§å›æŠ¥çš„åŠ¨ä½œ\n        action = greed_policy(Qtable, state)\n    else: # æ¢ç´¢ éšä¾¿é€‰ä¸€ä¸ªåŠ¨ä½œ çœ‹çœ‹èƒ½ä¸èƒ½æœ‰æ›´å¥½çš„\n        action = env.action_space.sample()\n        \n    return action","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:26.961188Z","iopub.execute_input":"2023-09-19T06:09:26.961856Z","iopub.status.idle":"2023-09-19T06:09:26.969126Z","shell.execute_reply.started":"2023-09-19T06:09:26.96181Z","shell.execute_reply":"2023-09-19T06:09:26.968011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# å®šä¹‰è¶…å‚æ•°\n# ç¡®ä¿ agent å¯ä»¥æ¢ç´¢è¶³å¤Ÿçš„çŠ¶æ€ç©ºé—´æ¥å­¦ä¹ ä¸€ä¸ªå¥½çš„è¿‘ä¼¼å€¼ -> éœ€è¦ epsilon é€æ¸ä¸‹é™\n# å¦‚æœ epsilon å‡å°çš„å¤ªå¿«(è¡°å‡ç‡å¤ªé«˜) å°±ä¼šå‡ºç° agent è¢«å¡ä½çš„æƒ…å†µ (è¿™ä¸ªæŒ‡çš„åº”è¯¥æ˜¯é™·å…¥å±€éƒ¨æœ€ä¼˜å§)\n\n# è®­ç»ƒå‚æ•°\nn_training_episodes = 10000 # æ€»å¾—è®­ç»ƒæ¬¡æ•°\nlearning_rate = 0.7 # å­¦ä¹ ç‡\n\n# è¯„ä¼°å‚æ•°\nn_eval_episodes = 100 # æµ‹è¯•é›†æ€»æ•°\n\n# ç¯å¢ƒå‚æ•°\nenv_id = \"FrozenLake-v1\" # ç¯å¢ƒåç§°\nmax_steps = 99 # æ¯ä¸ª episodes ä¸­çš„æœ€å¤§æ­¥æ•°\ngamma = 0.95 # æŠ˜æ‰£ç‡\neval_seed = [] # ç¯å¢ƒè¯„ä¼°ç§å­ï¼Ÿ\n\n# Exploration å‚æ•°\nmax_epsilon = 1.0 # å¼€å§‹çš„æ¢ç´¢æ¦‚ç‡\nmin_epsilon = 0.05 # æœ€å°æ¢ç´¢æ¦‚ç‡\ndecay_rate = 0.005 # æ¢ç´¢æŒ‡æ•°çš„è¡°å‡ç‡","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:29.816226Z","iopub.execute_input":"2023-09-19T06:09:29.817418Z","iopub.status.idle":"2023-09-19T06:09:29.823928Z","shell.execute_reply.started":"2023-09-19T06:09:29.817376Z","shell.execute_reply":"2023-09-19T06:09:29.822491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# åˆ›å»ºå¾ªç¯è®­ç»ƒå‡½æ•°\ndef train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n    for episode in tqdm(range(n_training_episodes)): # tqdm ç”¨äºåœ¨å¾ªç¯ä¸­æ˜¾ç¤ºè¿›åº¦æ¡\n        # å‡å° epsilon å› ä¸ºæˆ‘ä»¬éœ€è¦è¶Šæ¥è¶Šå°‘çš„æ¢ç´¢\n        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n        # reset the enviroment\n        state, info = env.reset()\n        step = 0\n        terminate = False\n        truncated = False\n        \n        # é‡å¤\n        for step in range(max_steps):\n            # ä½¿ç”¨ epsilon greedy policy æ¥é€‰æ‹©åŠ¨ä½œ\n            action = epsilon_greedy_policy(Qtable, state, epsilon)\n            \n            # é‡‡å–è¡ŒåŠ¨\n            new_state, reward, terminated, truncated, info = env.step(action)\n            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n            Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n            \n            # å¦‚æœç»ˆæ­¢ æˆ–è€… å½“å‰ episodes å®Œæˆäº†\n            if terminated or truncated:\n                break\n            \n            # æ›´æ–°çŠ¶æ€\n            state = new_state\n    return Qtable","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:11:42.019649Z","iopub.execute_input":"2023-09-19T06:11:42.02009Z","iopub.status.idle":"2023-09-19T06:11:42.031499Z","shell.execute_reply.started":"2023-09-19T06:11:42.020059Z","shell.execute_reply":"2023-09-19T06:11:42.029988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# è®­ç»ƒ Q-learing Agent frozenlake","metadata":{}},{"cell_type":"code","source":"Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:21:43.177531Z","iopub.execute_input":"2023-09-19T06:21:43.178047Z","iopub.status.idle":"2023-09-19T06:21:46.823447Z","shell.execute_reply.started":"2023-09-19T06:21:43.178013Z","shell.execute_reply":"2023-09-19T06:21:46.821978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# é¡ºä¾¿æ‰“å°ä¸€ä¸‹\nQtable_frozenlake","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:21:48.291318Z","iopub.execute_input":"2023-09-19T06:21:48.291984Z","iopub.status.idle":"2023-09-19T06:21:48.300589Z","shell.execute_reply.started":"2023-09-19T06:21:48.291949Z","shell.execute_reply":"2023-09-19T06:21:48.299431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# è¯„ä¼°å‡½æ•°","metadata":{}},{"cell_type":"code","source":"def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param max_steps: Maximum number of steps per episode\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param Q: The Q-table\n  :param seed: The evaluation seed array (for taxi-v3)\n  \"\"\"\n  episode_rewards = []\n  for episode in tqdm(range(n_eval_episodes)):\n    if seed:\n      state, info = env.reset(seed=seed[episode])\n    else:\n      state, info = env.reset()\n    step = 0\n    truncated = False\n    terminated = False\n    total_rewards_ep = 0\n\n    for step in range(max_steps):\n      # Take the action (index) that have the maximum expected future reward given that state\n      action = greedy_policy(Q, state)\n      new_state, reward, terminated, truncated, info = env.step(action)\n      total_rewards_ep += reward\n\n      if terminated or truncated:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n\n  return mean_reward, std_reward","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:21:54.223053Z","iopub.execute_input":"2023-09-19T06:21:54.223526Z","iopub.status.idle":"2023-09-19T06:21:54.233962Z","shell.execute_reply.started":"2023-09-19T06:21:54.223493Z","shell.execute_reply":"2023-09-19T06:21:54.232353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate our Agent\nmean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\nprint(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:21:56.886095Z","iopub.execute_input":"2023-09-19T06:21:56.886625Z","iopub.status.idle":"2023-09-19T06:21:56.95275Z","shell.execute_reply.started":"2023-09-19T06:21:56.886586Z","shell.execute_reply":"2023-09-19T06:21:56.951481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Do not modify this code","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import HfApi, snapshot_download\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport json","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:25:06.084856Z","iopub.execute_input":"2023-09-19T06:25:06.085351Z","iopub.status.idle":"2023-09-19T06:25:06.313739Z","shell.execute_reply.started":"2023-09-19T06:25:06.085316Z","shell.execute_reply":"2023-09-19T06:25:06.312356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def record_video(env, Qtable, out_directory, fps=1):\n  \"\"\"\n  Generate a replay video of the agent\n  :param env\n  :param Qtable: Qtable of our agent\n  :param out_directory\n  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n  \"\"\"\n  images = []\n  terminated = False\n  truncated = False\n  state, info = env.reset(seed=random.randint(0,500))\n  img = env.render()\n  images.append(img)\n  while not terminated or truncated:\n    # Take the action (index) that have the maximum expected future reward given that state\n    action = np.argmax(Qtable[state][:])\n    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n    img = env.render()\n    images.append(img)\n  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:25:11.839601Z","iopub.execute_input":"2023-09-19T06:25:11.840654Z","iopub.status.idle":"2023-09-19T06:25:11.850039Z","shell.execute_reply.started":"2023-09-19T06:25:11.840614Z","shell.execute_reply":"2023-09-19T06:25:11.848589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def push_to_hub(\n    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the Hub\n\n    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n    :param env\n    :param video_fps: how many frame per seconds to record our video replay\n    (with taxi-v3 and frozenlake-v1 we use 1)\n    :param local_repo_path: where the local repository is\n    \"\"\"\n    _, repo_name = repo_id.split(\"/\")\n\n    eval_env = env\n    api = HfApi()\n\n    # Step 1: Create the repo\n    repo_url = api.create_repo(\n        repo_id=repo_id,\n        exist_ok=True,\n    )\n\n    # Step 2: Download files\n    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n\n    # Step 3: Save the model\n    if env.spec.kwargs.get(\"map_name\"):\n        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n            model[\"slippery\"] = False\n\n    # Pickle the model\n    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n        pickle.dump(model, f)\n\n    # Step 4: Evaluate the model and build JSON with evaluation metrics\n    mean_reward, std_reward = evaluate_agent(\n        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n    )\n\n    evaluate_data = {\n        \"env_id\": model[\"env_id\"],\n        \"mean_reward\": mean_reward,\n        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n        \"eval_datetime\": datetime.datetime.now().isoformat()\n    }\n\n    # Write a JSON file called \"results.json\" that will contain the\n    # evaluation results\n    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n        json.dump(evaluate_data, outfile)\n\n    # Step 5: Create the model card\n    env_name = model[\"env_id\"]\n    if env.spec.kwargs.get(\"map_name\"):\n        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n\n    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n        env_name += \"-\" + \"no_slippery\"\n\n    metadata = {}\n    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=repo_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_name,\n        dataset_id=env_name,\n    )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    model_card = f\"\"\"\n  # **Q-Learning** Agent playing1 **{env_id}**\n  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n\n  ## Usage\n\n  ```python\n\n  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n\n  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n  env = gym.make(model[\"env_id\"])\n  ```\n  \"\"\"\n\n    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n\n    readme_path = repo_local_path / \"README.md\"\n    readme = \"\"\n    print(readme_path.exists())\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\n    # Step 6: Record a video\n    video_path = repo_local_path / \"replay.mp4\"\n    record_video(env, model[\"qtable\"], video_path, video_fps)\n\n    # Step 7. Push everything to the Hub\n    api.upload_folder(\n        repo_id=repo_id,\n        folder_path=repo_local_path,\n        path_in_repo=\".\",\n    )\n\n    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:25:14.272934Z","iopub.execute_input":"2023-09-19T06:25:14.273359Z","iopub.status.idle":"2023-09-19T06:25:14.294959Z","shell.execute_reply.started":"2023-09-19T06:25:14.273326Z","shell.execute_reply":"2023-09-19T06:25:14.293378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ä¸Šä¼ åˆ° hugging hub è¯·ä¿®æ”¹username å’Œ repo_name","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:25:18.95217Z","iopub.execute_input":"2023-09-19T06:25:18.953257Z","iopub.status.idle":"2023-09-19T06:25:18.988586Z","shell.execute_reply.started":"2023-09-19T06:25:18.953216Z","shell.execute_reply":"2023-09-19T06:25:18.987147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n\n    \"qtable\": Qtable_frozenlake\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:25:37.341547Z","iopub.execute_input":"2023-09-19T06:25:37.342042Z","iopub.status.idle":"2023-09-19T06:25:37.349301Z","shell.execute_reply.started":"2023-09-19T06:25:37.342006Z","shell.execute_reply":"2023-09-19T06:25:37.348221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"username = \"Solitary12138\" # FILL THIS\nrepo_name = \"Frozen-Lake\"\npush_to_hub(\n    repo_id=f\"{username}/{repo_name}\",\n    model=model,\n    env=env)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:25:59.299798Z","iopub.execute_input":"2023-09-19T06:25:59.300326Z","iopub.status.idle":"2023-09-19T06:26:01.544248Z","shell.execute_reply.started":"2023-09-19T06:25:59.300287Z","shell.execute_reply":"2023-09-19T06:26:01.542866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# è®­ç»ƒ Taxi-v3","metadata":{}},{"cell_type":"code","source":"env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:49:27.538144Z","iopub.execute_input":"2023-09-19T06:49:27.539483Z","iopub.status.idle":"2023-09-19T06:49:27.556836Z","shell.execute_reply.started":"2023-09-19T06:49:27.539438Z","shell.execute_reply":"2023-09-19T06:49:27.555616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* å‡ºç§Ÿè½¦æœ‰ 25 ä¸ªå¯èƒ½ä½ç½® ä¹˜å®¢æœ‰ 5 ä¸ªå¯èƒ½ä½ç½®(åŒ…æ‹¬ä¹˜å®¢åœ¨å‡ºç§Ÿè½¦ä¸Šçš„ä½ç½®)å’Œ 4 ä¸ªç›®çš„åœ°ä½ç½® å› æ­¤æœ‰ 500 ä¸ªç¦»æ•£çŠ¶æ€","metadata":{}},{"cell_type":"code","source":"state_space = env.observation_space.n\nprint(\"There are \", state_space, \" possible state\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:49:36.912131Z","iopub.execute_input":"2023-09-19T06:49:36.912588Z","iopub.status.idle":"2023-09-19T06:49:36.918837Z","shell.execute_reply.started":"2023-09-19T06:49:36.912553Z","shell.execute_reply":"2023-09-19T06:49:36.91784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"action_space = env.action_space.n\nprint(\"There are \", action_space, \" possible action\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:49:38.984877Z","iopub.execute_input":"2023-09-19T06:49:38.985575Z","iopub.status.idle":"2023-09-19T06:49:38.991916Z","shell.execute_reply.started":"2023-09-19T06:49:38.985537Z","shell.execute_reply":"2023-09-19T06:49:38.990328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The action space (the set of possible actions the agent can take) is discrete with **6 actions available ğŸ®**:\n\n- 0: move south\n- 1: move north\n- 2: move east\n- 3: move west\n- 4: pickup passenger\n- 5: drop off passenger\n\nReward function ğŸ’°:\n\n- -1 per step unless other reward is triggered.\n- +20 delivering passenger.\n- -10 executing â€œpickupâ€ and â€œdrop-offâ€ actions illegally.","metadata":{}},{"cell_type":"code","source":"# åˆ›å»º Q-table 500 * 6\nQtable_taxi = initialize_q_table(state_space, action_space)\nprint(Qtable_taxi)\nprint(\"Q-table shape: \", Qtable_taxi.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:49:42.006556Z","iopub.execute_input":"2023-09-19T06:49:42.007085Z","iopub.status.idle":"2023-09-19T06:49:42.015381Z","shell.execute_reply.started":"2023-09-19T06:49:42.007044Z","shell.execute_reply":"2023-09-19T06:49:42.014071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# å®šä¹‰è¶…å‚æ•°\n# Training parameters\nn_training_episodes = 25000   # Total training episodes\nlearning_rate = 0.7           # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100        # Total number of test episodes\n\n# DO NOT MODIFY EVAL_SEED\neval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n                                                          # Each seed has a specific starting state\n\n# Environment parameters\nenv_id = \"Taxi-v3\"           # Name of the environment\nmax_steps = 99               # Max steps per episode\ngamma = 0.95                 # Discounting rate\n\n# Exploration parameters\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.05           # Minimum exploration probability\ndecay_rate = 0.005            # Exponential decay rate for exploration prob","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:49:45.517812Z","iopub.execute_input":"2023-09-19T06:49:45.518306Z","iopub.status.idle":"2023-09-19T06:49:45.53153Z","shell.execute_reply.started":"2023-09-19T06:49:45.518269Z","shell.execute_reply":"2023-09-19T06:49:45.5301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# è®­ç»ƒ Taxi-v3","metadata":{}},{"cell_type":"code","source":"Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\nQtable_taxi","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:49:48.338401Z","iopub.execute_input":"2023-09-19T06:49:48.338869Z","iopub.status.idle":"2023-09-19T06:50:12.364107Z","shell.execute_reply.started":"2023-09-19T06:49:48.33883Z","shell.execute_reply":"2023-09-19T06:50:12.362549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ä¿å­˜æ¨¡å‹åˆ° hugging hub","metadata":{}},{"cell_type":"code","source":"model = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n\n    \"qtable\": Qtable_taxi\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:50:24.160392Z","iopub.execute_input":"2023-09-19T06:50:24.160837Z","iopub.status.idle":"2023-09-19T06:50:24.168181Z","shell.execute_reply.started":"2023-09-19T06:50:24.160805Z","shell.execute_reply":"2023-09-19T06:50:24.166985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"username = \"Solitary12138\" # FILL THIS\nrepo_name = \"Taxi-V3\" # FILL THIS\npush_to_hub(\n    repo_id=f\"{username}/{repo_name}\",\n    model=model,\n    env=env)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:50:30.010761Z","iopub.execute_input":"2023-09-19T06:50:30.012038Z","iopub.status.idle":"2023-09-19T06:50:32.722008Z","shell.execute_reply.started":"2023-09-19T06:50:30.01197Z","shell.execute_reply":"2023-09-19T06:50:32.720428Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
