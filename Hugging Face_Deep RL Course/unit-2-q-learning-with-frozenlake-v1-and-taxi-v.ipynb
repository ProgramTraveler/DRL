{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/chhelp/unit-2-q-learning-with-frozenlake-v1-and-taxi-v?scriptVersionId=143490188\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 通过 Q-learning 简单训练模型\n\n* gymnasium: 包含 FrozenLake-v1 ⛄ 和 Taxi-v3 🚕 环境\n* pygame: 用于 FrozenLake-v1 和 Taxi-v3 UI\n* numpy: 用于处理我们的 Q 表。","metadata":{}},{"cell_type":"code","source":"!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:07:54.659534Z","iopub.execute_input":"2023-09-19T06:07:54.66Z","iopub.status.idle":"2023-09-19T06:08:21.027299Z","shell.execute_reply.started":"2023-09-19T06:07:54.659966Z","shell.execute_reply":"2023-09-19T06:08:21.025884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install -y python3-opengl\n!apt install ffmpeg xvfb\n!pip3 install pyvirtualdisplay","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:08:21.029921Z","iopub.execute_input":"2023-09-19T06:08:21.030286Z","iopub.status.idle":"2023-09-19T06:08:54.877064Z","shell.execute_reply.started":"2023-09-19T06:08:21.030252Z","shell.execute_reply":"2023-09-19T06:08:54.875484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 创建虚拟屏幕\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:01.575434Z","iopub.execute_input":"2023-09-19T06:09:01.575996Z","iopub.status.idle":"2023-09-19T06:09:02.06681Z","shell.execute_reply.started":"2023-09-19T06:09:01.575947Z","shell.execute_reply":"2023-09-19T06:09:02.065448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport gymnasium as gym\nimport random # 生成随机数\nimport imageio # 生成重播视频\nimport tqdm\n\nimport pickle5 as pickle\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:04.419447Z","iopub.execute_input":"2023-09-19T06:09:04.419951Z","iopub.status.idle":"2023-09-19T06:09:05.125752Z","shell.execute_reply.started":"2023-09-19T06:09:04.419913Z","shell.execute_reply":"2023-09-19T06:09:05.124286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 开始训练  Frozen Lake ⛄\n\nWe're going to train our Q-Learning agent **to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)**.\n\nWe can have two sizes of environment:\n\n- `map_name=\"4x4\"`: a 4x4 grid version\n- `map_name=\"8x8\"`: a 8x8 grid version\n\n\nThe environment has two modes:\n\n- `is_slippery=False`: The agent always moves **in the intended direction** due to the non-slippery nature of the frozen lake (deterministic).\n- `is_slippery=True`: The agent **may not always move in the intended direction** due to the slippery nature of the frozen lake (stochastic).","metadata":{}},{"cell_type":"code","source":"# 选择 4x4 的地图\n# 冰湖选择防滑 保证 agent 始终朝着预期方向前进\n# render_mode 指定如何可视化环境 rgb_array -> 返回环境当前状态的单个帧 帧是一个 np.ndarray 形状为 (x, y, 3) 表示 x*y 像素图像的 RGB 值\nenv = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:08.726459Z","iopub.execute_input":"2023-09-19T06:09:08.726904Z","iopub.status.idle":"2023-09-19T06:09:08.757633Z","shell.execute_reply.started":"2023-09-19T06:09:08.726871Z","shell.execute_reply":"2023-09-19T06:09:08.755821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 你也可以自定义网格\n\n\n```python\ndesc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\ngym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n```\n\n* 在这里我使用的是默认环境","metadata":{}},{"cell_type":"code","source":"# 查看创建的环境\nprint(\"_____OBSERVATION SPACE_____\")\nprint(\"Observation Space\", env.observation_space)\nprint(\"Sample observation\", env.observation_space.sample()) # 对状态随机采样","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:11.690412Z","iopub.execute_input":"2023-09-19T06:09:11.691135Z","iopub.status.idle":"2023-09-19T06:09:11.702793Z","shell.execute_reply.started":"2023-09-19T06:09:11.691079Z","shell.execute_reply":"2023-09-19T06:09:11.700639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 输出结果中 Observation Space Discrete(16) 是一个整数 它表示代理的当前位置 current_row * ncols + current_col(其中 row 和 col 都是从 0 开始)\n* 在 4x4 的地图中 目标位置通过 $3 * 4 + 3 = 15$ 计算得到 可能的观察值(observations)数量取决于地图的大小 4x4 地图中有 16个可能的观察值","metadata":{}},{"cell_type":"code","source":"# antion\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample()) # 随机采样一个动作","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:13.678903Z","iopub.execute_input":"2023-09-19T06:09:13.67936Z","iopub.status.idle":"2023-09-19T06:09:13.687184Z","shell.execute_reply.started":"2023-09-19T06:09:13.679326Z","shell.execute_reply":"2023-09-19T06:09:13.685615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*  离散的动作空间\n\n    -with 4 actions available 🎮:\n    - 0: GO LEFT\n    - 1: GO DOWN\n    - 2: GO RIGHT\n    - 3: GO UP\n\n* 奖励函数\n\n    -Reward function 💰:\n    - Reach goal: +1\n    - Reach hole: 0\n    - Reach frozen: 0","metadata":{}},{"cell_type":"code","source":"state_space = env.observation_space.n # 状态空间\nprint(\"There are \", state_space, \"possible states\")\n\naction_space = env.action_space.n\nprint(\"There are \", action_space, \"possible actions\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:16.758112Z","iopub.execute_input":"2023-09-19T06:09:16.758518Z","iopub.status.idle":"2023-09-19T06:09:16.766088Z","shell.execute_reply.started":"2023-09-19T06:09:16.758473Z","shell.execute_reply":"2023-09-19T06:09:16.764399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 创建并初始化 Q-table\ndef initialize_q_table(state_space, action_space):\n    Qtable = np.zeros((state_space, action_space)) # 创建一个 初始为 0 大小为 state_space * action_space 的向量\n    return Qtable","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:19.357051Z","iopub.execute_input":"2023-09-19T06:09:19.357433Z","iopub.status.idle":"2023-09-19T06:09:19.363884Z","shell.execute_reply.started":"2023-09-19T06:09:19.357404Z","shell.execute_reply":"2023-09-19T06:09:19.362376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Qtable_frozenlake = initialize_q_table(state_space, action_space)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:21.42235Z","iopub.execute_input":"2023-09-19T06:09:21.422743Z","iopub.status.idle":"2023-09-19T06:09:21.428903Z","shell.execute_reply.started":"2023-09-19T06:09:21.422714Z","shell.execute_reply":"2023-09-19T06:09:21.427447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 定义 greedy 策略 其实就是贪心\ndef greedy_policy(Qtable, state):\n    action = np.argmax(Qtable[state][:]) # 返回在当前状态下 采取哪个行动(获取奖励最大的那个行动)\n    return action","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:10:57.47236Z","iopub.execute_input":"2023-09-19T06:10:57.472873Z","iopub.status.idle":"2023-09-19T06:10:57.479621Z","shell.execute_reply.started":"2023-09-19T06:10:57.472833Z","shell.execute_reply":"2023-09-19T06:10:57.478338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The idea with epsilon-greedy:\n\n- With *probability 1 - ɛ* : **we do exploitation** (i.e. our agent selects the action with the highest state-action pair value).\n\n- With *probability ɛ*: we do **exploration** (trying a random action).","metadata":{}},{"cell_type":"code","source":"# 定义 epsilon-greedy 策略\n# 处理 探索(exploration)和利用(exploitation)之间的平衡\ndef epsilon_greedy_policy(Qtable, state, epsilon):\n    # 在 0 和 1 之间随机产生一个数字\n    random_num = random.uniform(0, 1)\n    \n    if random_num > epsilon: # 利用 其实就是利用之前的 Q-table 中的信息\n        # 选择当前状态下可以获得最大回报的动作\n        action = greed_policy(Qtable, state)\n    else: # 探索 随便选一个动作 看看能不能有更好的\n        action = env.action_space.sample()\n        \n    return action","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:26.961188Z","iopub.execute_input":"2023-09-19T06:09:26.961856Z","iopub.status.idle":"2023-09-19T06:09:26.969126Z","shell.execute_reply.started":"2023-09-19T06:09:26.96181Z","shell.execute_reply":"2023-09-19T06:09:26.968011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 定义超参数\n# 确保 agent 可以探索足够的状态空间来学习一个好的近似值 -> 需要 epsilon 逐渐下降\n# 如果 epsilon 减小的太快(衰减率太高) 就会出现 agent 被卡住的情况 (这个指的应该是陷入局部最优吧)\n\n# 训练参数\nn_training_episodes = 10000 # 总得训练次数\nlearning_rate = 0.7 # 学习率\n\n# 评估参数\nn_eval_episodes = 100 # 测试集总数\n\n# 环境参数\nenv_id = \"FrozenLake-v1\" # 环境名称\nmax_steps = 99 # 每个 episodes 中的最大步数\ngamma = 0.95 # 折扣率\neval_seed = [] # 环境评估种子？\n\n# Exploration 参数\nmax_epsilon = 1.0 # 开始的探索概率\nmin_epsilon = 0.05 # 最小探索概率\ndecay_rate = 0.005 # 探索指数的衰减率","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:09:29.816226Z","iopub.execute_input":"2023-09-19T06:09:29.817418Z","iopub.status.idle":"2023-09-19T06:09:29.823928Z","shell.execute_reply.started":"2023-09-19T06:09:29.817376Z","shell.execute_reply":"2023-09-19T06:09:29.822491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 创建循环训练函数\ndef train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n    for episode in tqdm(range(n_training_episodes)): # tqdm 用于在循环中显示进度条\n        # 减小 epsilon 因为我们需要越来越少的探索\n        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n        # reset the enviroment\n        state, info = env.reset()\n        step = 0\n        terminate = False\n        truncated = False\n        \n        # 重复\n        for step in range(max_steps):\n            # 使用 epsilon greedy policy 来选择动作\n            action = epsilon_greedy_policy(Qtable, state, epsilon)\n            \n            # 采取行动\n            new_state, reward, terminated, truncated, info = env.step(action)\n            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n            Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n            \n            # 如果终止 或者 当前 episodes 完成了\n            if terminated or truncated:\n                break\n            \n            # 更新状态\n            state = new_state\n    return Qtable","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:11:42.019649Z","iopub.execute_input":"2023-09-19T06:11:42.02009Z","iopub.status.idle":"2023-09-19T06:11:42.031499Z","shell.execute_reply.started":"2023-09-19T06:11:42.020059Z","shell.execute_reply":"2023-09-19T06:11:42.029988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 训练 Q-learing Agent frozenlake","metadata":{}},{"cell_type":"code","source":"Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:21:43.177531Z","iopub.execute_input":"2023-09-19T06:21:43.178047Z","iopub.status.idle":"2023-09-19T06:21:46.823447Z","shell.execute_reply.started":"2023-09-19T06:21:43.178013Z","shell.execute_reply":"2023-09-19T06:21:46.821978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 顺便打印一下\nQtable_frozenlake","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:21:48.291318Z","iopub.execute_input":"2023-09-19T06:21:48.291984Z","iopub.status.idle":"2023-09-19T06:21:48.300589Z","shell.execute_reply.started":"2023-09-19T06:21:48.291949Z","shell.execute_reply":"2023-09-19T06:21:48.299431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 评估函数","metadata":{}},{"cell_type":"code","source":"def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param max_steps: Maximum number of steps per episode\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param Q: The Q-table\n  :param seed: The evaluation seed array (for taxi-v3)\n  \"\"\"\n  episode_rewards = []\n  for episode in tqdm(range(n_eval_episodes)):\n    if seed:\n      state, info = env.reset(seed=seed[episode])\n    else:\n      state, info = env.reset()\n    step = 0\n    truncated = False\n    terminated = False\n    total_rewards_ep = 0\n\n    for step in range(max_steps):\n      # Take the action (index) that have the maximum expected future reward given that state\n      action = greedy_policy(Q, state)\n      new_state, reward, terminated, truncated, info = env.step(action)\n      total_rewards_ep += reward\n\n      if terminated or truncated:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n\n  return mean_reward, std_reward","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:21:54.223053Z","iopub.execute_input":"2023-09-19T06:21:54.223526Z","iopub.status.idle":"2023-09-19T06:21:54.233962Z","shell.execute_reply.started":"2023-09-19T06:21:54.223493Z","shell.execute_reply":"2023-09-19T06:21:54.232353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate our Agent\nmean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\nprint(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:21:56.886095Z","iopub.execute_input":"2023-09-19T06:21:56.886625Z","iopub.status.idle":"2023-09-19T06:21:56.95275Z","shell.execute_reply.started":"2023-09-19T06:21:56.886586Z","shell.execute_reply":"2023-09-19T06:21:56.951481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Do not modify this code","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import HfApi, snapshot_download\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport json","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:25:06.084856Z","iopub.execute_input":"2023-09-19T06:25:06.085351Z","iopub.status.idle":"2023-09-19T06:25:06.313739Z","shell.execute_reply.started":"2023-09-19T06:25:06.085316Z","shell.execute_reply":"2023-09-19T06:25:06.312356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def record_video(env, Qtable, out_directory, fps=1):\n  \"\"\"\n  Generate a replay video of the agent\n  :param env\n  :param Qtable: Qtable of our agent\n  :param out_directory\n  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n  \"\"\"\n  images = []\n  terminated = False\n  truncated = False\n  state, info = env.reset(seed=random.randint(0,500))\n  img = env.render()\n  images.append(img)\n  while not terminated or truncated:\n    # Take the action (index) that have the maximum expected future reward given that state\n    action = np.argmax(Qtable[state][:])\n    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n    img = env.render()\n    images.append(img)\n  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:25:11.839601Z","iopub.execute_input":"2023-09-19T06:25:11.840654Z","iopub.status.idle":"2023-09-19T06:25:11.850039Z","shell.execute_reply.started":"2023-09-19T06:25:11.840614Z","shell.execute_reply":"2023-09-19T06:25:11.848589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def push_to_hub(\n    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the Hub\n\n    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n    :param env\n    :param video_fps: how many frame per seconds to record our video replay\n    (with taxi-v3 and frozenlake-v1 we use 1)\n    :param local_repo_path: where the local repository is\n    \"\"\"\n    _, repo_name = repo_id.split(\"/\")\n\n    eval_env = env\n    api = HfApi()\n\n    # Step 1: Create the repo\n    repo_url = api.create_repo(\n        repo_id=repo_id,\n        exist_ok=True,\n    )\n\n    # Step 2: Download files\n    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n\n    # Step 3: Save the model\n    if env.spec.kwargs.get(\"map_name\"):\n        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n            model[\"slippery\"] = False\n\n    # Pickle the model\n    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n        pickle.dump(model, f)\n\n    # Step 4: Evaluate the model and build JSON with evaluation metrics\n    mean_reward, std_reward = evaluate_agent(\n        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n    )\n\n    evaluate_data = {\n        \"env_id\": model[\"env_id\"],\n        \"mean_reward\": mean_reward,\n        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n        \"eval_datetime\": datetime.datetime.now().isoformat()\n    }\n\n    # Write a JSON file called \"results.json\" that will contain the\n    # evaluation results\n    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n        json.dump(evaluate_data, outfile)\n\n    # Step 5: Create the model card\n    env_name = model[\"env_id\"]\n    if env.spec.kwargs.get(\"map_name\"):\n        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n\n    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n        env_name += \"-\" + \"no_slippery\"\n\n    metadata = {}\n    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=repo_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_name,\n        dataset_id=env_name,\n    )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    model_card = f\"\"\"\n  # **Q-Learning** Agent playing1 **{env_id}**\n  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n\n  ## Usage\n\n  ```python\n\n  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n\n  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n  env = gym.make(model[\"env_id\"])\n  ```\n  \"\"\"\n\n    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n\n    readme_path = repo_local_path / \"README.md\"\n    readme = \"\"\n    print(readme_path.exists())\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\n    # Step 6: Record a video\n    video_path = repo_local_path / \"replay.mp4\"\n    record_video(env, model[\"qtable\"], video_path, video_fps)\n\n    # Step 7. Push everything to the Hub\n    api.upload_folder(\n        repo_id=repo_id,\n        folder_path=repo_local_path,\n        path_in_repo=\".\",\n    )\n\n    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:25:14.272934Z","iopub.execute_input":"2023-09-19T06:25:14.273359Z","iopub.status.idle":"2023-09-19T06:25:14.294959Z","shell.execute_reply.started":"2023-09-19T06:25:14.273326Z","shell.execute_reply":"2023-09-19T06:25:14.293378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 上传到 hugging hub 请修改username 和 repo_name","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:25:18.95217Z","iopub.execute_input":"2023-09-19T06:25:18.953257Z","iopub.status.idle":"2023-09-19T06:25:18.988586Z","shell.execute_reply.started":"2023-09-19T06:25:18.953216Z","shell.execute_reply":"2023-09-19T06:25:18.987147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n\n    \"qtable\": Qtable_frozenlake\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:25:37.341547Z","iopub.execute_input":"2023-09-19T06:25:37.342042Z","iopub.status.idle":"2023-09-19T06:25:37.349301Z","shell.execute_reply.started":"2023-09-19T06:25:37.342006Z","shell.execute_reply":"2023-09-19T06:25:37.348221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"username = \"Solitary12138\" # FILL THIS\nrepo_name = \"Frozen-Lake\"\npush_to_hub(\n    repo_id=f\"{username}/{repo_name}\",\n    model=model,\n    env=env)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:25:59.299798Z","iopub.execute_input":"2023-09-19T06:25:59.300326Z","iopub.status.idle":"2023-09-19T06:26:01.544248Z","shell.execute_reply.started":"2023-09-19T06:25:59.300287Z","shell.execute_reply":"2023-09-19T06:26:01.542866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 训练 Taxi-v3","metadata":{}},{"cell_type":"code","source":"env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:49:27.538144Z","iopub.execute_input":"2023-09-19T06:49:27.539483Z","iopub.status.idle":"2023-09-19T06:49:27.556836Z","shell.execute_reply.started":"2023-09-19T06:49:27.539438Z","shell.execute_reply":"2023-09-19T06:49:27.555616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 出租车有 25 个可能位置 乘客有 5 个可能位置(包括乘客在出租车上的位置)和 4 个目的地位置 因此有 500 个离散状态","metadata":{}},{"cell_type":"code","source":"state_space = env.observation_space.n\nprint(\"There are \", state_space, \" possible state\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:49:36.912131Z","iopub.execute_input":"2023-09-19T06:49:36.912588Z","iopub.status.idle":"2023-09-19T06:49:36.918837Z","shell.execute_reply.started":"2023-09-19T06:49:36.912553Z","shell.execute_reply":"2023-09-19T06:49:36.91784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"action_space = env.action_space.n\nprint(\"There are \", action_space, \" possible action\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:49:38.984877Z","iopub.execute_input":"2023-09-19T06:49:38.985575Z","iopub.status.idle":"2023-09-19T06:49:38.991916Z","shell.execute_reply.started":"2023-09-19T06:49:38.985537Z","shell.execute_reply":"2023-09-19T06:49:38.990328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The action space (the set of possible actions the agent can take) is discrete with **6 actions available 🎮**:\n\n- 0: move south\n- 1: move north\n- 2: move east\n- 3: move west\n- 4: pickup passenger\n- 5: drop off passenger\n\nReward function 💰:\n\n- -1 per step unless other reward is triggered.\n- +20 delivering passenger.\n- -10 executing “pickup” and “drop-off” actions illegally.","metadata":{}},{"cell_type":"code","source":"# 创建 Q-table 500 * 6\nQtable_taxi = initialize_q_table(state_space, action_space)\nprint(Qtable_taxi)\nprint(\"Q-table shape: \", Qtable_taxi.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:49:42.006556Z","iopub.execute_input":"2023-09-19T06:49:42.007085Z","iopub.status.idle":"2023-09-19T06:49:42.015381Z","shell.execute_reply.started":"2023-09-19T06:49:42.007044Z","shell.execute_reply":"2023-09-19T06:49:42.014071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 定义超参数\n# Training parameters\nn_training_episodes = 25000   # Total training episodes\nlearning_rate = 0.7           # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100        # Total number of test episodes\n\n# DO NOT MODIFY EVAL_SEED\neval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n                                                          # Each seed has a specific starting state\n\n# Environment parameters\nenv_id = \"Taxi-v3\"           # Name of the environment\nmax_steps = 99               # Max steps per episode\ngamma = 0.95                 # Discounting rate\n\n# Exploration parameters\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.05           # Minimum exploration probability\ndecay_rate = 0.005            # Exponential decay rate for exploration prob","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:49:45.517812Z","iopub.execute_input":"2023-09-19T06:49:45.518306Z","iopub.status.idle":"2023-09-19T06:49:45.53153Z","shell.execute_reply.started":"2023-09-19T06:49:45.518269Z","shell.execute_reply":"2023-09-19T06:49:45.5301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 训练 Taxi-v3","metadata":{}},{"cell_type":"code","source":"Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\nQtable_taxi","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:49:48.338401Z","iopub.execute_input":"2023-09-19T06:49:48.338869Z","iopub.status.idle":"2023-09-19T06:50:12.364107Z","shell.execute_reply.started":"2023-09-19T06:49:48.33883Z","shell.execute_reply":"2023-09-19T06:50:12.362549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 保存模型到 hugging hub","metadata":{}},{"cell_type":"code","source":"model = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n\n    \"qtable\": Qtable_taxi\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:50:24.160392Z","iopub.execute_input":"2023-09-19T06:50:24.160837Z","iopub.status.idle":"2023-09-19T06:50:24.168181Z","shell.execute_reply.started":"2023-09-19T06:50:24.160805Z","shell.execute_reply":"2023-09-19T06:50:24.166985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"username = \"Solitary12138\" # FILL THIS\nrepo_name = \"Taxi-V3\" # FILL THIS\npush_to_hub(\n    repo_id=f\"{username}/{repo_name}\",\n    model=model,\n    env=env)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T06:50:30.010761Z","iopub.execute_input":"2023-09-19T06:50:30.012038Z","iopub.status.idle":"2023-09-19T06:50:32.722008Z","shell.execute_reply.started":"2023-09-19T06:50:30.01197Z","shell.execute_reply":"2023-09-19T06:50:32.720428Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
