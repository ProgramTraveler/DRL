{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/chhelp/unit1-land-on-the-moon?scriptVersionId=143403174\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Hugging Face 的在月球上降落\n\n* 安装依赖 将安装多个依赖\n    * gymnasium[box2d] 包含 LunarLander-v2 环境\n    * stable-baselines3[extra] 深度强化学习库\n    * huggingface_sb3 Stable-baseline3 的附加代码 用于从 Hugging Face Hub 加载和上传模型","metadata":{}},{"cell_type":"code","source":"!apt install swig cmake","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:05:38.5743Z","iopub.execute_input":"2023-09-18T12:05:38.574788Z","iopub.status.idle":"2023-09-18T12:05:40.934301Z","shell.execute_reply.started":"2023-09-18T12:05:38.57473Z","shell.execute_reply":"2023-09-18T12:05:40.932999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:53:05.283843Z","iopub.execute_input":"2023-09-18T10:53:05.284222Z","iopub.status.idle":"2023-09-18T10:54:10.27278Z","shell.execute_reply.started":"2023-09-18T10:53:05.284182Z","shell.execute_reply":"2023-09-18T10:54:10.271517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 使用 Colab 生成重播视频 需要一个虚拟屏幕来渲染环境 从而记录帧\n* 以下单元格将安装虚拟屏幕库并创建运行虚拟屏幕","metadata":{}},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install -y python3-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip3 install pyvirtualdisplay","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:06:18.766495Z","iopub.execute_input":"2023-09-18T12:06:18.766971Z","iopub.status.idle":"2023-09-18T12:06:49.419196Z","shell.execute_reply.started":"2023-09-18T12:06:18.766931Z","shell.execute_reply":"2023-09-18T12:06:49.417943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 虚拟屏幕\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:07:44.222301Z","iopub.execute_input":"2023-09-18T12:07:44.223375Z","iopub.status.idle":"2023-09-18T12:07:44.324509Z","shell.execute_reply.started":"2023-09-18T12:07:44.223333Z","shell.execute_reply":"2023-09-18T12:07:44.323347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 导入 Huggingface_hub 包 以便能够上传和下载经过训练的模型","metadata":{}},{"cell_type":"code","source":"import gymnasium\n\nfrom huggingface_sb3 import load_from_hub, package_to_hub\nfrom huggingface_hub import notebook_login # 登录 Hugging Face 帐户以便能够将模型上传到 Hub\n\nfrom stable_baselines3 import PPO # 直接使用的 PPO 算法\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.monitor import Monitor","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:07:52.036464Z","iopub.execute_input":"2023-09-18T12:07:52.036884Z","iopub.status.idle":"2023-09-18T12:07:52.04328Z","shell.execute_reply.started":"2023-09-18T12:07:52.036849Z","shell.execute_reply":"2023-09-18T12:07:52.04195Z"},"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Gymnasium\n    * 使用 gymnasium.make() 来创建环境\n    * 使用 observation() = env.reset() 将环境重置为初始状态\n* 在每一个 step\n    * 使用我们的模型获取一个 action 在例子中 我们采用随机操作\n    * 使用 env.step(action) 在环境中执行这个 action 并得到\n        * observation: 新状态($S_(t + 1)$)\n        * reward: 执行 action 后获得的奖励\n        * terminated: episode 是否结束(agent 到达终止状态)\n        * truncated: 在新版本中引入 它指示时间限制或者 agent 是否超出环境范围\n        * info: 提供附加信息(取决于环境) 是一个字典\n* 如果 episoide 终止\n    * 使用 observation = env.reset() 将环境重置为初始环境","metadata":{}},{"cell_type":"code","source":"# 这只是针对上面步骤的一个列子\nimport gymnasium as gym\n\n# 创建一个名为 LunarLander-v2 的一个环境\nenv = gym.make(\"LunarLander-v2\")\n\n# 重置环境\nobservation, info = env.reset()\n\nfor _ in range(20):\n    # 采取一个随机动作\n    action = env.action_space.sample()\n    print(\"Action taken: \", action)\n    \n    # 在环境中采取这个动作 并获取 next_state reward terminated truncated info\n    observation, reward, terminated, truncated, info = env.step(action)\n    \n    # 如果游戏结束(这里指的是降落或者坠毁) 或者时间耗尽\n    if terminated or truncated:\n        # 重置环境\n        print(\"Enviroment is reset\")\n        observation, info = env.reset()\n\n# 清理环境\nenv.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 训练 agent(月球着陆器) 正确登录月球","metadata":{}},{"cell_type":"code","source":"# 创建环境\nenv = gym.make(\"LunarLander-v2\")\nenv.reset()\n\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"Observation Space Shape\", env.observation_space.shape)\nprint(\"Sample observation\", env.observation_space.sample()) # 获取随机的状态","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:55:52.834499Z","iopub.execute_input":"2023-09-18T10:55:52.834897Z","iopub.status.idle":"2023-09-18T10:55:52.845196Z","shell.execute_reply.started":"2023-09-18T10:55:52.834863Z","shell.execute_reply":"2023-09-18T10:55:52.843881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Observation Space Shape 是一个大小为 8 的向量 每个值都包含不同信息\n    - Horizontal pad coordinate (x)\n    - Vertical pad coordinate (y)\n    - Horizontal speed (x)\n    - Vertical speed (y)\n    - Angle\n    - Angular speed\n    - If the left leg contact point has touched the land (boolean)\n    - If the right leg contact point has touched the land (boolean)","metadata":{}},{"cell_type":"code","source":"print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample()) # 获取随机动作","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:55:54.992641Z","iopub.execute_input":"2023-09-18T10:55:54.993973Z","iopub.status.idle":"2023-09-18T10:55:55.000814Z","shell.execute_reply.started":"2023-09-18T10:55:54.993928Z","shell.execute_reply":"2023-09-18T10:55:54.99952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 动作空间(agent 可能采取的一组动作)是离散的 有四个可用的动作\n    - Action 0: Do nothing,\n    - Action 1: Fire left orientation engine,\n    - Action 2: Fire the main engine,\n    - Action 3: Fire right orientation engine.\n* 对每一个 step 的奖励设置\n    - Is increased/decreased the closer/further the lander is to the landing pad.\n    -  Is increased/decreased the slower/faster the lander is moving.\n    - Is decreased the more the lander is tilted (angle not horizontal).\n    - Is increased by 10 points for each leg that is in contact with the ground.\n    - Is decreased by 0.03 points each frame a side engine is firing.\n    - Is decreased by 0.3 points each frame the main engine is firing.\n* 对每个 episode 着陆器因坠毁或者安全着陆分别获得 -100 和 +100 的额外奖励\n* 如果一个 episode 的得分至少为 200 分则视为一个解决方案","metadata":{}},{"cell_type":"code","source":"# 矢量化环境\n# 创建一个由 16 个环境组成的矢量化环境(一种将多个独立环境堆叠到单个环境中的方法)\n# 这样在训练中就会有更多样化的体验\n# 创建环境\nenv = make_vec_env('LunarLander-v2', n_envs=16)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:56:19.376475Z","iopub.execute_input":"2023-09-18T10:56:19.376861Z","iopub.status.idle":"2023-09-18T10:56:19.397579Z","shell.execute_reply.started":"2023-09-18T10:56:19.376828Z","shell.execute_reply":"2023-09-18T10:56:19.396501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 使用第一个深度强化学习库 Stable Baselines3(SB3)\n* SB3 是通过 PyTorch 实现的强化学习组\n* 在当前代码中 使用的是 SB3 中的 PPO 算法","metadata":{}},{"cell_type":"code","source":"# 创建环境\nenv = gym.make(\"LunarLander-v2\")\n\n'''\n# 定义使用的 agent 并实例化该模型\nmodel = PPO('MlpPolicy', env, verbose=1)\n# 训练模型 并定义训练的 timesteps\nmodel.learn(total_timesteps=int(2e5))\n'''\n\n# 添加一些参数 来加速训练的过程\nmodel = PPO(\n    policy = 'MlpPolicy',\n    env = env,\n    n_steps = 1024,\n    batch_size = 64,\n    n_epochs = 4,\n    gamma = 0.999,\n    gae_lambda = 0.98,\n    ent_coef = 0.01,\n    verbose=1)\n\n# 训练 agent 1,000,000 timesteps\nmodel.learn(total_timesteps=1000000)\n# 保存模型\nmodel_name = \"ppo-LunarLander-v2\"\nmodel.save(model_name)","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 评估训练的代理\n* 在 Stable-Baselines3 提供了一种方法 evaluate_policy","metadata":{}},{"cell_type":"code","source":"eval_env = Monitor(gym.make(\"LunarLander-v2\"))\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:32:15.085698Z","iopub.execute_input":"2023-09-18T11:32:15.086735Z","iopub.status.idle":"2023-09-18T11:32:18.088381Z","shell.execute_reply.started":"2023-09-18T11:32:15.086692Z","shell.execute_reply":"2023-09-18T11:32:18.086906Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
