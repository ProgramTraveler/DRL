{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hugging Face 的在月球上降落\n\n* 安装依赖 将安装多个依赖\n    * gymnasium[box2d] 包含 LunarLander-v2 环境\n    * stable-baselines3[extra] 深度强化学习库\n    * huggingface_sb3 Stable-baseline3 的附加代码 用于从 Hugging Face Hub 加载和上传模型","metadata":{}},{"cell_type":"code","source":"!apt install swig cmake","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:52:57.411164Z","iopub.execute_input":"2023-09-18T10:52:57.411817Z","iopub.status.idle":"2023-09-18T10:53:05.280825Z","shell.execute_reply.started":"2023-09-18T10:52:57.411750Z","shell.execute_reply":"2023-09-18T10:53:05.279552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:53:05.283843Z","iopub.execute_input":"2023-09-18T10:53:05.284222Z","iopub.status.idle":"2023-09-18T10:54:10.272780Z","shell.execute_reply.started":"2023-09-18T10:53:05.284182Z","shell.execute_reply":"2023-09-18T10:54:10.271517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 使用 Colab 生成重播视频 需要一个虚拟屏幕来渲染环境 从而记录帧\n* 以下单元格将安装虚拟屏幕库并创建运行虚拟屏幕","metadata":{}},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install -y python3-openg1\n!apt install ffmpeg\n!apt install xvfb\n!pip3 install pyvirtualdisplay","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:54:10.275887Z","iopub.execute_input":"2023-09-18T10:54:10.276484Z","iopub.status.idle":"2023-09-18T10:54:34.558326Z","shell.execute_reply.started":"2023-09-18T10:54:10.276443Z","shell.execute_reply":"2023-09-18T10:54:34.556909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 虚拟屏幕\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:54:34.562037Z","iopub.execute_input":"2023-09-18T10:54:34.562522Z","iopub.status.idle":"2023-09-18T10:54:35.756859Z","shell.execute_reply.started":"2023-09-18T10:54:34.562482Z","shell.execute_reply":"2023-09-18T10:54:35.755836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 导入 Huggingface_hub 包 以便能够上传和下载经过训练的模型","metadata":{}},{"cell_type":"code","source":"import gymnasium\n\nfrom huggingface_sb3 import load_from_hub, package_to_hub\nfrom huggingface_hub import notebook_login # 登录 Hugging Face 帐户以便能够将模型上传到 Hub\n\nfrom stable_baselines3 import PPO # 直接使用的 PPO 算法\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.monitor import Monitor","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:55:46.452887Z","iopub.execute_input":"2023-09-18T10:55:46.453267Z","iopub.status.idle":"2023-09-18T10:55:46.460352Z","shell.execute_reply.started":"2023-09-18T10:55:46.453237Z","shell.execute_reply":"2023-09-18T10:55:46.458309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Gymnasium\n    * 使用 gymnasium.make() 来创建环境\n    * 使用 observation() = env.reset() 将环境重置为初始状态\n* 在每一个 step\n    * 使用我们的模型获取一个 action 在例子中 我们采用随机操作\n    * 使用 env.step(action) 在环境中执行这个 action 并得到\n        * observation: 新状态($S_(t + 1)$)\n        * reward: 执行 action 后获得的奖励\n        * terminated: episode 是否结束(agent 到达终止状态)\n        * truncated: 在新版本中引入 它指示时间限制或者 agent 是否超出环境范围\n        * info: 提供附加信息(取决于环境) 是一个字典\n* 如果 episoide 终止\n    * 使用 observation = env.reset() 将环境重置为初始环境","metadata":{}},{"cell_type":"code","source":"# 这只是针对上面步骤的一个列子\nimport gymnasium as gym\n\n# 创建一个名为 LunarLander-v2 的一个环境\nenv = gym.make(\"LunarLander-v2\")\n\n# 重置环境\nobservation, info = env.reset()\n\nfor _ in range(20):\n    # 采取一个随机动作\n    action = env.action_space.sample()\n    print(\"Action taken: \", action)\n    \n    # 在环境中采取这个动作 并获取 next_state reward terminated truncated info\n    observation, reward, terminated, truncated, info = env.step(action)\n    \n    # 如果游戏结束(这里指的是降落或者坠毁) 或者时间耗尽\n    if terminated or truncated:\n        # 重置环境\n        print(\"Enviroment is reset\")\n        observation, info = env.reset()\n\n# 清理环境\nenv.close()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:54:57.143804Z","iopub.execute_input":"2023-09-18T10:54:57.144950Z","iopub.status.idle":"2023-09-18T10:54:57.224202Z","shell.execute_reply.started":"2023-09-18T10:54:57.144911Z","shell.execute_reply":"2023-09-18T10:54:57.222951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 训练 agent(月球着陆器) 正确登录月球","metadata":{}},{"cell_type":"code","source":"# 创建环境\nenv = gym.make(\"LunarLander-v2\")\nenv.reset()\n\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"Observation Space Shape\", env.observation_space.shape)\nprint(\"Sample observation\", env.observation_space.sample()) # 获取随机的状态","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:55:52.834499Z","iopub.execute_input":"2023-09-18T10:55:52.834897Z","iopub.status.idle":"2023-09-18T10:55:52.845196Z","shell.execute_reply.started":"2023-09-18T10:55:52.834863Z","shell.execute_reply":"2023-09-18T10:55:52.843881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Observation Space Shape 是一个大小为 8 的向量 每个值都包含不同信息\n    - Horizontal pad coordinate (x)\n    - Vertical pad coordinate (y)\n    - Horizontal speed (x)\n    - Vertical speed (y)\n    - Angle\n    - Angular speed\n    - If the left leg contact point has touched the land (boolean)\n    - If the right leg contact point has touched the land (boolean)","metadata":{}},{"cell_type":"code","source":"print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample()) # 获取随机动作","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:55:54.992641Z","iopub.execute_input":"2023-09-18T10:55:54.993973Z","iopub.status.idle":"2023-09-18T10:55:55.000814Z","shell.execute_reply.started":"2023-09-18T10:55:54.993928Z","shell.execute_reply":"2023-09-18T10:55:54.999520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 动作空间(agent 可能采取的一组动作)是离散的 有四个可用的动作\n    - Action 0: Do nothing,\n    - Action 1: Fire left orientation engine,\n    - Action 2: Fire the main engine,\n    - Action 3: Fire right orientation engine.\n* 对每一个 step 的奖励设置\n    - Is increased/decreased the closer/further the lander is to the landing pad.\n    -  Is increased/decreased the slower/faster the lander is moving.\n    - Is decreased the more the lander is tilted (angle not horizontal).\n    - Is increased by 10 points for each leg that is in contact with the ground.\n    - Is decreased by 0.03 points each frame a side engine is firing.\n    - Is decreased by 0.3 points each frame the main engine is firing.\n* 对每个 episode 着陆器因坠毁或者安全着陆分别获得 -100 和 +100 的额外奖励\n* 如果一个 episode 的得分至少为 200 分则视为一个解决方案","metadata":{}},{"cell_type":"code","source":"# 矢量化环境\n# 创建一个由 16 个环境组成的矢量化环境(一种将多个独立环境堆叠到单个环境中的方法)\n# 这样在训练中就会有更多样化的体验\n# 创建环境\nenv = make_vec_env('LunarLander-v2', n_envs=16)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:56:19.376475Z","iopub.execute_input":"2023-09-18T10:56:19.376861Z","iopub.status.idle":"2023-09-18T10:56:19.397579Z","shell.execute_reply.started":"2023-09-18T10:56:19.376828Z","shell.execute_reply":"2023-09-18T10:56:19.396501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 使用第一个深度强化学习库 Stable Baselines3(SB3)\n* SB3 是通过 PyTorch 实现的强化学习组\n* 在当前代码中 使用的是 SB3 中的 PPO 算法","metadata":{}},{"cell_type":"code","source":"# 创建环境\nenv = gym.make(\"LunarLander-v2\")\n\n'''\n# 定义使用的 agent 并实例化该模型\nmodel = PPO('MlpPolicy', env, verbose=1)\n# 训练模型 并定义训练的 timesteps\nmodel.learn(total_timesteps=int(2e5))\n'''\n\n# 添加一些参数 来加速训练的过程\nmodel = PPO(\n    policy = 'MlpPolicy',\n    env = env,\n    n_steps = 1024,\n    batch_size = 64,\n    n_epochs = 4,\n    gamma = 0.999,\n    gae_lambda = 0.98,\n    ent_coef = 0.01,\n    verbose=1)\n\n# 训练 agent 1,000,000 timesteps\nmodel.learn(total_timesteps=1000000)\n# 保存模型\nmodel_name = \"ppo-LunarLander-v2\"\nmodel.save(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:56:23.439832Z","iopub.execute_input":"2023-09-18T10:56:23.441014Z","iopub.status.idle":"2023-09-18T11:28:48.537555Z","shell.execute_reply.started":"2023-09-18T10:56:23.440973Z","shell.execute_reply":"2023-09-18T11:28:48.536366Z"},"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 评估训练的代理\n* 在 Stable-Baselines3 提供了一种方法 evaluate_policy","metadata":{}},{"cell_type":"code","source":"eval_env = Monitor(gym.make(\"LunarLander-v2\"))\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:32:15.085698Z","iopub.execute_input":"2023-09-18T11:32:15.086735Z","iopub.status.idle":"2023-09-18T11:32:18.088381Z","shell.execute_reply.started":"2023-09-18T11:32:15.086692Z","shell.execute_reply":"2023-09-18T11:32:18.086906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 发布到 hub 中","metadata":{}},{"cell_type":"code","source":"notebook_login()\n!git config --global credential.helper store","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:32:21.889698Z","iopub.execute_input":"2023-09-18T11:32:21.890753Z","iopub.status.idle":"2023-09-18T11:32:23.054903Z","shell.execute_reply.started":"2023-09-18T11:32:21.890714Z","shell.execute_reply":"2023-09-18T11:32:23.053514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's fill the `package_to_hub` function:\n- `model`: our trained model.\n- `model_name`: the name of the trained model that we defined in `model_save`\n- `model_architecture`: the model architecture we used, in our case PPO\n- `env_id`: the name of the environment, in our case `LunarLander-v2`\n- `eval_env`: the evaluation environment defined in eval_env\n- `repo_id`: the name of the Hugging Face Hub Repository that will be created/updated `(repo_id = {username}/{repo_name})`\n\n💡 **A good name is {username}/{model_architecture}-{env_id}**\n\n- `commit_message`: message of the commit","metadata":{}},{"cell_type":"code","source":"import gymnasium as gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.env_util import make_vec_env\n\nfrom huggingface_sb3 import package_to_hub\n\n# PLACE the variables you've just defined two cells above\n# Define the name of the environment\nenv_id = \"LunarLander-v2\"\n\n# TODO: Define the model architecture we used\nmodel_architecture = \"PPO\"\n\n## Define a repo_id\n## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n## CHANGE WITH YOUR REPO ID\nrepo_id = \"Solitary12138/Load_Moon\" # Change with your repo id, you can't push with mine 😄\n\n## Define the commit message\ncommit_message = \"Upload PPO LunarLander-v2 trained agent\"\n\n# Create the evaluation env and set the render_mode=\"rgb_array\"\neval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n\n# PLACE the package_to_hub function you've just filled here\npackage_to_hub(model=model, # Our trained model\n               model_name=model_name, # The name of our trained model\n               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n               env_id=env_id, # Name of the environment\n               eval_env=eval_env, # Evaluation Environment\n               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n               commit_message=commit_message)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:35:31.466416Z","iopub.execute_input":"2023-09-18T11:35:31.466950Z","iopub.status.idle":"2023-09-18T11:35:36.049709Z","shell.execute_reply.started":"2023-09-18T11:35:31.466913Z","shell.execute_reply":"2023-09-18T11:35:36.047127Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
